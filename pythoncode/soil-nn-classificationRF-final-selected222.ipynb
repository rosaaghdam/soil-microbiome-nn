{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69988add",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "import os\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pandas import ExcelWriter\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a7fbaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab79a226",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data_train,data_val,cv):  \n",
    "    x_column_list = data_train.drop(columns=['y_b']).columns  \n",
    "    percent_label=[round(100*len(np.where(data_train['y_b']==0)[0])/len(data_train)),round(100*len(np.where(data_val['y_b']==0)[0])/len(data_val))]  \n",
    "    #Classification RF  \n",
    "    pipeRF = Pipeline([('classifier', [RandomForestClassifier()])])  \n",
    "    param_grid = [  \n",
    "    {'classifier' : [RandomForestClassifier()], \n",
    "    'classifier__n_estimators': [100, 200],\n",
    "    'classifier__min_samples_split': [8, 10],\n",
    "    'classifier__min_samples_leaf': [3, 4, 5],\n",
    "     'classifier__max_depth': [80, 90],\n",
    "    'classifier__criterion':('gini','entropy'),  \n",
    "    'classifier__class_weight':('balanced','auto')}]  \n",
    "    clf = GridSearchCV(pipeRF, param_grid = param_grid, cv = cv, n_jobs=-1, scoring='f1_weighted')  \n",
    "    # Fit on data  \n",
    "    clf.fit(data_train[x_column_list],data_train['y_b'])  \n",
    "    best_clf=clf.best_estimator_  \n",
    "    y_valid=best_clf.predict(data_val[x_column_list])  \n",
    "    report_All = classification_report(data_val['y_b'],y_valid,output_dict=True)  \n",
    "    dAll=pd.DataFrame(report_All).transpose()  \n",
    "    return dAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fc9ebe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv=RepeatedKFold(n_splits=10,n_repeats=3, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e31e4d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path2 ='/Users/rosa/Desktop/ALLWork/Madison/Project/Soil-nn/Code/python code local/Main Data Files/disease_response'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "473b8124",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_response='/Users/rosa/Desktop/ALLWork/Madison/Project/Soil-nn/Code/python code local/Main Data Files/disease_response/'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f53ab77",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_list2 = []\n",
    "#reading files in folder response\n",
    "for root, dirs, files in os.walk(data_path2, topdown=False):\n",
    "    for path in dirs:\n",
    "        path_list2.append(path)\n",
    "#reading sheet name       \n",
    "wb = openpyxl.load_workbook(data_path2+'/'+path+'/feature_selection.xlsx')\n",
    "sheet_list = wb.sheetnames\n",
    "\n",
    "results_dic = dict.fromkeys(sheet_list)\n",
    "\n",
    "for sheet_name in results_dic.keys():\n",
    "    temp_df = pd.DataFrame(columns=path_list2, index=range(0,700))\n",
    "    for folder in path_list2:\n",
    "        data_temp = pd.read_excel(data_path2+'/'+folder+'/feature_selection.xlsx', sheet_name=sheet_name)\n",
    "        temp_df[folder].iloc[\n",
    "            range(0, len(data_temp['Unnamed: 0'].values))] = data_temp['Unnamed: 0'].values\n",
    "\n",
    "    results_dic[sheet_name] = temp_df\n",
    "    \n",
    "for key in results_dic.keys():\n",
    "    results_dic[key] = results_dic[key].iloc[range(0,max(30, round(len(data_temp)/3)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a75b9824",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/Users/rosa/Desktop/ALLWork/Madison/Project/Soil-nn/Code/python code local/Main Data Files/response_netcomi'\n",
    "\n",
    "path_list = []\n",
    "\n",
    "for root, dirs, files in os.walk(data_path, topdown=False):\n",
    "    for path in dirs:\n",
    "        path_list.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "477ac4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "list_level = ['Class', 'Family', 'Genus', 'Order', 'Phylum']\n",
    "\n",
    "all_data = dict.fromkeys(dirs)\n",
    "\n",
    "for folder in path_list:\n",
    "    \n",
    "    all_data[folder] = dict.fromkeys(list_level)\n",
    "    temp_path = data_path+'/'+folder\n",
    "    temp_file_list = os.listdir(temp_path)\n",
    "    for file in temp_file_list:\n",
    "        if 'Count' in file:\n",
    "            level = file.split('_')[-1][:-4]\n",
    "            data_temp = pd.read_csv(data_path+'/'+folder+'/'+file,index_col=0)\n",
    "            data_temp.sort_values(by='dif_close', ascending=False, inplace=True)\n",
    "            #all_data[folder][level] = data_temp.iloc[range(0,max(40, round(len(data_temp)/3)))]\n",
    "            all_data[folder][level] = data_temp.iloc[range(0,max(30, round(len(data_temp)/3)))]\n",
    "            #print(level,\":\",max(30 ,round(len(data_temp)/3)))\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6236f413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class\n",
      "78\n",
      "7\n",
      "8\n",
      "8\n",
      "Family\n",
      "145\n",
      "9\n",
      "8\n",
      "9\n",
      "Genus\n",
      "239\n",
      "5\n",
      "8\n",
      "12\n",
      "Order\n",
      "132\n",
      "9\n",
      "6\n",
      "10\n",
      "Phylum\n",
      "41\n",
      "22\n",
      "20\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "list_level = ['Class', 'Family', 'Genus', 'Order', 'Phylum']\n",
    "SELECTED_FEATURE = dict.fromkeys(dirs)\n",
    "writer= pd.ExcelWriter(path_response+'important_features_score'+'.xlsx', engine='xlsxwriter') \n",
    "for col in range(0,len(list_level)):\n",
    "    level = list_level[col]\n",
    "    print(level)\n",
    "    SELECTED_FEATURE[level] = dict.fromkeys(dirs)\n",
    "    response_list = path_list\n",
    "    feature_list = []\n",
    "    for response in response_list:\n",
    "        feature_list.append(all_data[response][level].index.union(results_dic[level][response][results_dic[level][response].notnull()].values))\n",
    "    feature_list = [item for subitem in feature_list for item in subitem]\n",
    "    feature_list = np.unique(feature_list)\n",
    "    print(len(feature_list))\n",
    "    matrix_df = pd.DataFrame(columns = response_list, index = feature_list)\n",
    "    for response in response_list:\n",
    "        for feature in feature_list:\n",
    "                if (feature in all_data[response][level].index) & (feature in results_dic[level][response].values):\n",
    "                    matrix_df[response].loc[feature] = 3\n",
    "                elif(feature in all_data[response][level].index) &(feature not in results_dic[level][response].values):\n",
    "                    matrix_df[response].loc[feature] = 2##NetComi\n",
    "                elif (feature not in all_data[response][level].index) &(feature in results_dic[level][response].values):\n",
    "                    matrix_df[response].loc[feature] = 1##ML\n",
    "                else:\n",
    "                    matrix_df[response].loc[feature] = 0##NotMLnotNetcomi\n",
    "            \n",
    "        #print(matrix_df[response])\n",
    "#         ree=re[matrix_df[response]==3]\n",
    "#         print(ree)\n",
    "#         SELECTED_FEATURE[response][level]= ree.index     \n",
    "        SELECTED_FEATURE[level][response] = matrix_df[response][matrix_df[response]==3].index\n",
    "        print(SELECTED_FEATURE[level][response].shape[0])\n",
    "    matrix_df['Sum'] = 0\n",
    "    for index in matrix_df.index:\n",
    "        matrix_df['Sum'].loc[index] = sum(matrix_df.loc[index].values)\n",
    "    \n",
    "    matrix_df.sort_values(by='Sum', ascending=False, inplace=True)\n",
    "    matrix_df.to_excel(writer, sheet_name=level, index=True)\n",
    "writer.save()     \n",
    "    #matrix_df=matrix_df.iloc[1:30,:]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726a0441",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_tuber_scab\n",
      " 1 _ 3 .csv\n",
      " 4 _ 1 .csv\n",
      " 5 _ 1 .csv\n",
      " 1 _ 2 .csv\n",
      " 1 _ 4 .csv\n",
      " 3 _ 1 .csv\n",
      " 2 _ 1 .csv\n",
      " 2 _ 2 .csv\n",
      " 3 _ 2 .csv\n",
      " 4 _ 4 .csv\n",
      " 5 _ 4 .csv\n",
      " 2 _ 3 .csv\n",
      " 3 _ 3 .csv\n",
      " 1 _ 1 .csv\n",
      " 3 _ 4 .csv\n",
      " 2 _ 4 .csv\n",
      " 5 _ 3 .csv\n",
      " 4 _ 3 .csv\n",
      " 5 _ 2 .csv\n",
      " 4 _ 2 .csv\n",
      " 1 _ 3 .csv\n",
      " 4 _ 1 .csv\n",
      " 5 _ 1 .csv\n",
      " 1 _ 2 .csv\n",
      " 1 _ 4 .csv\n",
      " 3 _ 1 .csv\n",
      " 2 _ 1 .csv\n",
      " 2 _ 2 .csv\n",
      " 3 _ 2 .csv\n",
      " 4 _ 4 .csv\n",
      " 5 _ 4 .csv\n",
      " 2 _ 3 .csv\n",
      " 3 _ 3 .csv\n",
      " 1 _ 1 .csv\n",
      " 3 _ 4 .csv\n",
      " 2 _ 4 .csv\n",
      " 5 _ 3 .csv\n",
      " 4 _ 3 .csv\n",
      " 5 _ 2 .csv\n",
      " 4 _ 2 .csv\n",
      " 1 _ 3 .csv\n",
      " 4 _ 1 .csv\n",
      " 5 _ 1 .csv\n",
      " 1 _ 2 .csv\n",
      " 1 _ 4 .csv\n",
      " 3 _ 1 .csv\n",
      " 2 _ 1 .csv\n",
      " 2 _ 2 .csv\n",
      " 3 _ 2 .csv\n",
      " 4 _ 4 .csv\n",
      " 5 _ 4 .csv\n",
      " 2 _ 3 .csv\n",
      " 3 _ 3 .csv\n",
      " 1 _ 1 .csv\n",
      " 3 _ 4 .csv\n",
      " 2 _ 4 .csv\n",
      " 5 _ 3 .csv\n",
      " 4 _ 3 .csv\n",
      " 5 _ 2 .csv\n",
      " 4 _ 2 .csv\n",
      " 1 _ 3 .csv\n",
      " 4 _ 1 .csv\n",
      " 5 _ 1 .csv\n",
      " 1 _ 2 .csv\n",
      " 1 _ 4 .csv\n",
      " 3 _ 1 .csv\n",
      " 2 _ 1 .csv\n",
      " 2 _ 2 .csv\n",
      " 3 _ 2 .csv\n",
      " 4 _ 4 .csv\n",
      " 5 _ 4 .csv\n",
      " 2 _ 3 .csv\n",
      " 3 _ 3 .csv\n",
      " 1 _ 1 .csv\n",
      " 3 _ 4 .csv\n",
      " 2 _ 4 .csv\n",
      " 5 _ 3 .csv\n",
      " 4 _ 3 .csv\n",
      " 5 _ 2 .csv\n",
      " 4 _ 2 .csv\n",
      " 1 _ 3 .csv\n",
      " 4 _ 1 .csv\n",
      " 5 _ 1 .csv\n",
      " 1 _ 2 .csv\n",
      " 1 _ 4 .csv\n",
      " 3 _ 1 .csv\n",
      " 2 _ 1 .csv\n",
      " 2 _ 2 .csv\n",
      " 3 _ 2 .csv\n",
      " 4 _ 4 .csv\n",
      " 5 _ 4 .csv\n",
      " 2 _ 3 .csv\n",
      " 3 _ 3 .csv\n",
      " 1 _ 1 .csv\n",
      " 3 _ 4 .csv\n",
      " 2 _ 4 .csv\n",
      " 5 _ 3 .csv\n",
      " 4 _ 3 .csv\n",
      " 5 _ 2 .csv\n",
      " 4 _ 2 .csv\n",
      "no_tuber_scabpit\n",
      " 1 _ 3 .csv\n",
      " 4 _ 1 .csv\n",
      " 5 _ 1 .csv\n",
      " 1 _ 2 .csv\n",
      " 1 _ 4 .csv\n",
      " 3 _ 1 .csv\n",
      " 2 _ 1 .csv\n",
      " 2 _ 2 .csv\n",
      " 3 _ 2 .csv\n",
      " 4 _ 4 .csv\n",
      " 5 _ 4 .csv\n",
      " 2 _ 3 .csv\n",
      " 3 _ 3 .csv\n",
      " 1 _ 1 .csv\n",
      " 3 _ 4 .csv\n",
      " 2 _ 4 .csv\n",
      " 5 _ 3 .csv\n",
      " 4 _ 3 .csv\n",
      " 5 _ 2 .csv\n",
      " 4 _ 2 .csv\n",
      " 1 _ 3 .csv\n",
      " 4 _ 1 .csv\n",
      " 5 _ 1 .csv\n",
      " 1 _ 2 .csv\n",
      " 1 _ 4 .csv\n",
      " 3 _ 1 .csv\n",
      " 2 _ 1 .csv\n",
      " 2 _ 2 .csv\n",
      " 3 _ 2 .csv\n",
      " 4 _ 4 .csv\n",
      " 5 _ 4 .csv\n",
      " 2 _ 3 .csv\n",
      " 3 _ 3 .csv\n",
      " 1 _ 1 .csv\n",
      " 3 _ 4 .csv\n",
      " 2 _ 4 .csv\n",
      " 5 _ 3 .csv\n",
      " 4 _ 3 .csv\n",
      " 5 _ 2 .csv\n",
      " 4 _ 2 .csv\n",
      " 1 _ 3 .csv\n",
      " 4 _ 1 .csv\n",
      " 5 _ 1 .csv\n",
      " 1 _ 2 .csv\n",
      " 1 _ 4 .csv\n",
      " 3 _ 1 .csv\n",
      " 2 _ 1 .csv\n"
     ]
    }
   ],
   "source": [
    "for file_response in os.listdir(path_response):  \n",
    "    if (file_response != '.DS_Store') & (file_response != 'Icon\\r'):  \n",
    "        print(file_response)  \n",
    "        path_r= path_response+file_response  \n",
    "        os.chdir(path_r)  \n",
    "        for re in os.listdir(path_r):  \n",
    "            if re[0:8] == 'response':  \n",
    "                response = pd.read_csv(path_response+file_response+'/'+re)  \n",
    "                response.rename(columns={'Column1':'Link_ID',response.columns[1]:'y_b'}, inplace=True)  \n",
    "                #response.drop(columns=response.columns[2], inplace=True)  \n",
    "                #response=response.drop(columns='Variety2')  \n",
    "                path_x = '/Users/rosa/Desktop/ALLWork/Madison/Project/Soil-nn/Code/python code local/Main Data Files/normalized_data_sklearn/'  \n",
    "                writer= pd.ExcelWriter(path_r+'/'+'classification_RF_FS'+'.xlsx', engine='xlsxwriter')   \n",
    "                for file_folder in os.listdir(path_x):  \n",
    "                    if (file_folder[-4:] != '.csv') & (file_folder != '.DS_Store')& (file_folder != 'Icon\\r'):          \n",
    "                        path = path_x+file_folder  \n",
    "                        os.chdir(path)  \n",
    "                        file_list = []  \n",
    "                        tRF=pd.DataFrame()  \n",
    "                        tcluster=pd.DataFrame()  \n",
    "                        k=0  \n",
    "                        for file in os.listdir(path):  \n",
    "                            if (file[0] != 't') & (file[-4:] == '.csv') & (file != '.DS_Store')& (file_folder != 'Icon\\r'):  \n",
    "                                print(file)  \n",
    "                                file_list.append(file)  \n",
    "                                data_temp = pd.read_csv(file) \n",
    "                                data_temp.rename(columns={'Unnamed: 0':'Link_ID'}, inplace=True) \n",
    "                                A = [item for item in SELECTED_FEATURE[file_folder][file_response]]\n",
    "                                A.insert(0, \"Link_ID\")\n",
    "                                data_temp = data_temp[A]\n",
    "                                data_temp.rename(columns={'Unnamed: 0':'Link_ID'}, inplace=True)  \n",
    "                                data=pd.merge(response,data_temp,on='Link_ID')\n",
    "                                data.drop(columns = 'Link_ID',inplace=True)  \n",
    "                                data_train,data_val = train_test_split(data,train_size=0.8, random_state=42)  \n",
    "                                output = process_data(data_train,data_val,cv)  \n",
    "                                tRF[k]=pd.DataFrame(output['f1-score'].values)         \n",
    "                                k=k+1  \n",
    "                        tRF.to_excel(writer, sheet_name=file_folder, index=True)  \n",
    "                writer.save()                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b001e92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class\n",
      "93\n",
      "8\n",
      "12\n",
      "11\n",
      "9\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "7\n",
      "Family\n",
      "186\n",
      "11\n",
      "10\n",
      "13\n",
      "13\n",
      "14\n",
      "7\n",
      "13\n",
      "10\n",
      "8\n",
      "Genus\n",
      "306\n",
      "7\n",
      "11\n",
      "12\n",
      "11\n",
      "11\n",
      "10\n",
      "9\n",
      "11\n",
      "12\n",
      "Order\n",
      "168\n",
      "11\n",
      "14\n",
      "9\n",
      "11\n",
      "12\n",
      "8\n",
      "11\n",
      "10\n",
      "10\n",
      "Phylum\n",
      "42\n",
      "21\n",
      "18\n",
      "21\n",
      "19\n",
      "20\n",
      "20\n",
      "21\n",
      "21\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "list_level = ['Class', 'Family', 'Genus', 'Order', 'Phylum']\n",
    "SELECTED_FEATURE = dict.fromkeys(dirs)\n",
    "SELECTED_FEATURE_0 = dict.fromkeys(dirs)\n",
    "for col in range(0,len(list_level)):\n",
    "    level = list_level[col]\n",
    "    print(level)\n",
    "    SELECTED_FEATURE[level] = dict.fromkeys(dirs)\n",
    "    SELECTED_FEATURE_0[level] = dict.fromkeys(dirs)\n",
    "    response_list = path_list\n",
    "    feature_list = []\n",
    "    for response in response_list:\n",
    "        feature_list.append(all_data[response][level].index.union(results_dic[level][response][results_dic[level][response].notnull()].values))\n",
    "    feature_list = [item for subitem in feature_list for item in subitem]\n",
    "    feature_list = np.unique(feature_list)\n",
    "    print(len(feature_list))\n",
    "    matrix_df = pd.DataFrame(columns = response_list, index = feature_list)\n",
    "    for response in response_list:\n",
    "        for feature in feature_list:\n",
    "                if (feature in all_data[response][level].index) & (feature in results_dic[level][response].values):\n",
    "                    matrix_df[response].loc[feature] = 3\n",
    "                elif(feature in all_data[response][level].index) &(feature not in results_dic[level][response].values):\n",
    "                    matrix_df[response].loc[feature] = 2##NetComi\n",
    "                elif (feature not in all_data[response][level].index) &(feature in results_dic[level][response].values):\n",
    "                    matrix_df[response].loc[feature] = 1##ML\n",
    "                else:\n",
    "                    matrix_df[response].loc[feature] = 0##NotMLnotNetcomi\n",
    "            \n",
    "        #print(matrix_df[response])\n",
    "#         ree=re[matrix_df[response]==3]\n",
    "#         print(ree)\n",
    "#         SELECTED_FEATURE[response][level]= ree.index     \n",
    "        SELECTED_FEATURE[level][response] = matrix_df[response][matrix_df[response]==3].index\n",
    "        print(SELECTED_FEATURE[level][response].shape[0])\n",
    "        SELECTED_FEATURE_0[level][response] = matrix_df[response][matrix_df[response]==0].index[0:SELECTED_FEATURE[level][response].shape[0]]\n",
    "        #print(SELECTED_FEATURE_0[level][response].shape[0])\n",
    "    \n",
    "    matrix_df['Sum'] = 0\n",
    "    for index in matrix_df.index:\n",
    "        matrix_df['Sum'].loc[index] = sum(matrix_df.loc[index].values)\n",
    "    \n",
    "    matrix_df.sort_values(by='Sum', ascending=False, inplace=True)\n",
    "    #matrix_df=matrix_df.iloc[1:30,:]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfb883c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yield_per_meter\n",
      " 1 _ 3 .csv\n",
      " 4 _ 1 .csv\n",
      " 5 _ 1 .csv\n",
      " 1 _ 2 .csv\n",
      " 1 _ 4 .csv\n",
      " 3 _ 1 .csv\n",
      " 2 _ 1 .csv\n",
      " 2 _ 2 .csv\n",
      " 3 _ 2 .csv\n",
      " 4 _ 4 .csv\n",
      " 5 _ 4 .csv\n",
      " 2 _ 3 .csv\n",
      " 3 _ 3 .csv\n",
      " 1 _ 1 .csv\n",
      " 3 _ 4 .csv\n",
      " 2 _ 4 .csv\n",
      " 5 _ 3 .csv\n",
      " 4 _ 3 .csv\n",
      " 5 _ 2 .csv\n",
      " 4 _ 2 .csv\n",
      " 1 _ 3 .csv\n",
      " 4 _ 1 .csv\n",
      " 5 _ 1 .csv\n",
      " 1 _ 2 .csv\n",
      " 1 _ 4 .csv\n",
      " 3 _ 1 .csv\n",
      " 2 _ 1 .csv\n",
      " 2 _ 2 .csv\n",
      " 3 _ 2 .csv\n",
      " 4 _ 4 .csv\n",
      " 5 _ 4 .csv\n",
      " 2 _ 3 .csv\n",
      " 3 _ 3 .csv\n",
      " 1 _ 1 .csv\n",
      " 3 _ 4 .csv\n",
      " 2 _ 4 .csv\n",
      " 5 _ 3 .csv\n",
      " 4 _ 3 .csv\n",
      " 5 _ 2 .csv\n",
      " 4 _ 2 .csv\n",
      " 1 _ 3 .csv\n",
      " 4 _ 1 .csv\n",
      " 5 _ 1 .csv\n",
      " 1 _ 2 .csv\n",
      " 1 _ 4 .csv\n",
      " 3 _ 1 .csv\n",
      " 2 _ 1 .csv\n",
      " 2 _ 2 .csv\n",
      " 3 _ 2 .csv\n",
      " 4 _ 4 .csv\n",
      " 5 _ 4 .csv\n",
      " 2 _ 3 .csv\n",
      " 3 _ 3 .csv\n",
      " 1 _ 1 .csv\n",
      " 3 _ 4 .csv\n",
      " 2 _ 4 .csv\n",
      " 5 _ 3 .csv\n",
      " 4 _ 3 .csv\n",
      " 5 _ 2 .csv\n",
      " 4 _ 2 .csv\n",
      " 1 _ 3 .csv\n",
      " 4 _ 1 .csv\n",
      " 5 _ 1 .csv\n",
      " 1 _ 2 .csv\n",
      " 1 _ 4 .csv\n",
      " 3 _ 1 .csv\n",
      " 2 _ 1 .csv\n",
      " 2 _ 2 .csv\n",
      " 3 _ 2 .csv\n",
      " 4 _ 4 .csv\n",
      " 5 _ 4 .csv\n",
      " 2 _ 3 .csv\n",
      " 3 _ 3 .csv\n",
      " 1 _ 1 .csv\n",
      " 3 _ 4 .csv\n",
      " 2 _ 4 .csv\n",
      " 5 _ 3 .csv\n",
      " 4 _ 3 .csv\n",
      " 5 _ 2 .csv\n",
      " 4 _ 2 .csv\n",
      " 1 _ 3 .csv\n",
      " 4 _ 1 .csv\n",
      " 5 _ 1 .csv\n",
      " 1 _ 2 .csv\n",
      " 1 _ 4 .csv\n",
      " 3 _ 1 .csv\n",
      " 2 _ 1 .csv\n",
      " 2 _ 2 .csv\n",
      " 3 _ 2 .csv\n",
      " 4 _ 4 .csv\n",
      " 5 _ 4 .csv\n",
      " 2 _ 3 .csv\n",
      " 3 _ 3 .csv\n",
      " 1 _ 1 .csv\n",
      " 3 _ 4 .csv\n",
      " 2 _ 4 .csv\n",
      " 5 _ 3 .csv\n",
      " 4 _ 3 .csv\n",
      " 5 _ 2 .csv\n",
      " 4 _ 2 .csv\n",
      "scab_severity\n",
      " 1 _ 3 .csv\n",
      " 4 _ 1 .csv\n",
      " 5 _ 1 .csv\n",
      " 1 _ 2 .csv\n",
      " 1 _ 4 .csv\n",
      " 3 _ 1 .csv\n",
      " 2 _ 1 .csv\n",
      " 2 _ 2 .csv\n",
      " 3 _ 2 .csv\n",
      " 4 _ 4 .csv\n",
      " 5 _ 4 .csv\n",
      " 2 _ 3 .csv\n",
      " 3 _ 3 .csv\n",
      " 1 _ 1 .csv\n",
      " 3 _ 4 .csv\n",
      " 2 _ 4 .csv\n",
      " 5 _ 3 .csv\n",
      " 4 _ 3 .csv\n",
      " 5 _ 2 .csv\n",
      " 4 _ 2 .csv\n",
      " 1 _ 3 .csv\n",
      " 4 _ 1 .csv\n",
      " 5 _ 1 .csv\n",
      " 1 _ 2 .csv\n",
      " 1 _ 4 .csv\n",
      " 3 _ 1 .csv\n",
      " 2 _ 1 .csv\n",
      " 2 _ 2 .csv\n",
      " 3 _ 2 .csv\n",
      " 4 _ 4 .csv\n",
      " 5 _ 4 .csv\n",
      " 2 _ 3 .csv\n",
      " 3 _ 3 .csv\n",
      " 1 _ 1 .csv\n",
      " 3 _ 4 .csv\n",
      " 2 _ 4 .csv\n",
      " 5 _ 3 .csv\n",
      " 4 _ 3 .csv\n",
      " 5 _ 2 .csv\n",
      " 4 _ 2 .csv\n",
      " 1 _ 3 .csv\n",
      " 4 _ 1 .csv\n",
      " 5 _ 1 .csv\n",
      " 1 _ 2 .csv\n",
      " 1 _ 4 .csv\n",
      " 3 _ 1 .csv\n",
      " 2 _ 1 .csv\n",
      " 2 _ 2 .csv\n",
      " 3 _ 2 .csv\n",
      " 4 _ 4 .csv\n",
      " 5 _ 4 .csv\n",
      " 2 _ 3 .csv\n",
      " 3 _ 3 .csv\n",
      " 1 _ 1 .csv\n",
      " 3 _ 4 .csv\n",
      " 2 _ 4 .csv\n",
      " 5 _ 3 .csv\n",
      " 4 _ 3 .csv\n",
      " 5 _ 2 .csv\n",
      " 4 _ 2 .csv\n",
      " 1 _ 3 .csv\n",
      " 4 _ 1 .csv\n",
      " 5 _ 1 .csv\n",
      " 1 _ 2 .csv\n",
      " 1 _ 4 .csv\n",
      " 3 _ 1 .csv\n",
      " 2 _ 1 .csv\n",
      " 2 _ 2 .csv\n",
      " 3 _ 2 .csv\n",
      " 4 _ 4 .csv\n",
      " 5 _ 4 .csv\n",
      " 2 _ 3 .csv\n",
      " 3 _ 3 .csv\n",
      " 1 _ 1 .csv\n",
      " 3 _ 4 .csv\n",
      " 2 _ 4 .csv\n",
      " 5 _ 3 .csv\n",
      " 4 _ 3 .csv\n",
      " 5 _ 2 .csv\n",
      " 4 _ 2 .csv\n",
      " 1 _ 3 .csv\n",
      " 4 _ 1 .csv\n",
      " 5 _ 1 .csv\n",
      " 1 _ 2 .csv\n",
      " 1 _ 4 .csv\n",
      " 3 _ 1 .csv\n",
      " 2 _ 1 .csv\n",
      " 2 _ 2 .csv\n",
      " 3 _ 2 .csv\n",
      " 4 _ 4 .csv\n",
      " 5 _ 4 .csv\n",
      " 2 _ 3 .csv\n",
      " 3 _ 3 .csv\n",
      " 1 _ 1 .csv\n",
      " 3 _ 4 .csv\n",
      " 2 _ 4 .csv\n",
      " 5 _ 3 .csv\n",
      " 4 _ 3 .csv\n",
      " 5 _ 2 .csv\n",
      " 4 _ 2 .csv\n",
      "no_tuber_scab\n",
      " 1 _ 3 .csv\n",
      " 4 _ 1 .csv\n",
      " 5 _ 1 .csv\n",
      " 1 _ 2 .csv\n",
      " 1 _ 4 .csv\n",
      " 3 _ 1 .csv\n",
      " 2 _ 1 .csv\n",
      " 2 _ 2 .csv\n",
      " 3 _ 2 .csv\n",
      " 4 _ 4 .csv\n",
      " 5 _ 4 .csv\n",
      " 2 _ 3 .csv\n",
      " 3 _ 3 .csv\n",
      " 1 _ 1 .csv\n",
      " 3 _ 4 .csv\n",
      " 2 _ 4 .csv\n",
      " 5 _ 3 .csv\n",
      " 4 _ 3 .csv\n",
      " 5 _ 2 .csv\n",
      " 4 _ 2 .csv\n",
      " 1 _ 3 .csv\n",
      " 4 _ 1 .csv\n",
      " 5 _ 1 .csv\n",
      " 1 _ 2 .csv\n",
      " 1 _ 4 .csv\n",
      " 3 _ 1 .csv\n",
      " 2 _ 1 .csv\n",
      " 2 _ 2 .csv\n",
      " 3 _ 2 .csv\n",
      " 4 _ 4 .csv\n",
      " 5 _ 4 .csv\n",
      " 2 _ 3 .csv\n",
      " 3 _ 3 .csv\n",
      " 1 _ 1 .csv\n",
      " 3 _ 4 .csv\n",
      " 2 _ 4 .csv\n",
      " 5 _ 3 .csv\n",
      " 4 _ 3 .csv\n",
      " 5 _ 2 .csv\n",
      " 4 _ 2 .csv\n",
      " 1 _ 3 .csv\n",
      " 4 _ 1 .csv\n",
      " 5 _ 1 .csv\n",
      " 1 _ 2 .csv\n",
      " 1 _ 4 .csv\n",
      " 3 _ 1 .csv\n",
      " 2 _ 1 .csv\n",
      " 2 _ 2 .csv\n",
      " 3 _ 2 .csv\n",
      " 4 _ 4 .csv\n",
      " 5 _ 4 .csv\n",
      " 2 _ 3 .csv\n",
      " 3 _ 3 .csv\n",
      " 1 _ 1 .csv\n",
      " 3 _ 4 .csv\n",
      " 2 _ 4 .csv\n",
      " 5 _ 3 .csv\n",
      " 4 _ 3 .csv\n",
      " 5 _ 2 .csv\n",
      " 4 _ 2 .csv\n",
      " 1 _ 3 .csv\n",
      " 4 _ 1 .csv\n",
      " 5 _ 1 .csv\n",
      " 1 _ 2 .csv\n",
      " 1 _ 4 .csv\n",
      " 3 _ 1 .csv\n",
      " 2 _ 1 .csv\n",
      " 2 _ 2 .csv\n",
      " 3 _ 2 .csv\n",
      " 4 _ 4 .csv\n",
      " 5 _ 4 .csv\n",
      " 2 _ 3 .csv\n",
      " 3 _ 3 .csv\n",
      " 1 _ 1 .csv\n",
      " 3 _ 4 .csv\n",
      " 2 _ 4 .csv\n",
      " 5 _ 3 .csv\n",
      " 4 _ 3 .csv\n",
      " 5 _ 2 .csv\n",
      " 4 _ 2 .csv\n",
      " 1 _ 3 .csv\n",
      " 4 _ 1 .csv\n",
      " 5 _ 1 .csv\n",
      " 1 _ 2 .csv\n",
      " 1 _ 4 .csv\n",
      " 3 _ 1 .csv\n",
      " 2 _ 1 .csv\n",
      " 2 _ 2 .csv\n",
      " 3 _ 2 .csv\n",
      " 4 _ 4 .csv\n",
      " 5 _ 4 .csv\n",
      " 2 _ 3 .csv\n",
      " 3 _ 3 .csv\n",
      " 1 _ 1 .csv\n",
      " 3 _ 4 .csv\n",
      " 2 _ 4 .csv\n",
      " 5 _ 3 .csv\n",
      " 4 _ 3 .csv\n",
      " 5 _ 2 .csv\n",
      " 4 _ 2 .csv\n",
      "pctg_black_scurf\n",
      " 1 _ 3 .csv\n",
      " 4 _ 1 .csv\n",
      " 5 _ 1 .csv\n",
      " 1 _ 2 .csv\n",
      " 1 _ 4 .csv\n",
      " 3 _ 1 .csv\n",
      " 2 _ 1 .csv\n",
      " 2 _ 2 .csv\n",
      " 3 _ 2 .csv\n",
      " 4 _ 4 .csv\n",
      " 5 _ 4 .csv\n",
      " 2 _ 3 .csv\n",
      " 3 _ 3 .csv\n",
      " 1 _ 1 .csv\n",
      " 3 _ 4 .csv\n",
      " 2 _ 4 .csv\n",
      " 5 _ 3 .csv\n",
      " 4 _ 3 .csv\n",
      " 5 _ 2 .csv\n",
      " 4 _ 2 .csv\n",
      " 1 _ 3 .csv\n",
      " 4 _ 1 .csv\n",
      " 5 _ 1 .csv\n",
      " 1 _ 2 .csv\n",
      " 1 _ 4 .csv\n",
      " 3 _ 1 .csv\n",
      " 2 _ 1 .csv\n",
      " 2 _ 2 .csv\n",
      " 3 _ 2 .csv\n",
      " 4 _ 4 .csv\n",
      " 5 _ 4 .csv\n",
      " 2 _ 3 .csv\n",
      " 3 _ 3 .csv\n",
      " 1 _ 1 .csv\n",
      " 3 _ 4 .csv\n",
      " 2 _ 4 .csv\n",
      " 5 _ 3 .csv\n",
      " 4 _ 3 .csv\n",
      " 5 _ 2 .csv\n",
      " 4 _ 2 .csv\n",
      " 1 _ 3 .csv\n",
      " 4 _ 1 .csv\n",
      " 5 _ 1 .csv\n",
      " 1 _ 2 .csv\n",
      " 1 _ 4 .csv\n",
      " 3 _ 1 .csv\n",
      " 2 _ 1 .csv\n",
      " 2 _ 2 .csv\n",
      " 3 _ 2 .csv\n",
      " 4 _ 4 .csv\n",
      " 5 _ 4 .csv\n",
      " 2 _ 3 .csv\n",
      " 3 _ 3 .csv\n",
      " 1 _ 1 .csv\n",
      " 3 _ 4 .csv\n",
      " 2 _ 4 .csv\n",
      " 5 _ 3 .csv\n",
      " 4 _ 3 .csv\n",
      " 5 _ 2 .csv\n",
      " 4 _ 2 .csv\n",
      " 1 _ 3 .csv\n",
      " 4 _ 1 .csv\n",
      " 5 _ 1 .csv\n",
      " 1 _ 2 .csv\n",
      " 1 _ 4 .csv\n",
      " 3 _ 1 .csv\n",
      " 2 _ 1 .csv\n",
      " 2 _ 2 .csv\n",
      " 3 _ 2 .csv\n",
      " 4 _ 4 .csv\n",
      " 5 _ 4 .csv\n",
      " 2 _ 3 .csv\n",
      " 3 _ 3 .csv\n",
      " 1 _ 1 .csv\n",
      " 3 _ 4 .csv\n",
      " 2 _ 4 .csv\n",
      " 5 _ 3 .csv\n",
      " 4 _ 3 .csv\n",
      " 5 _ 2 .csv\n",
      " 4 _ 2 .csv\n",
      " 1 _ 3 .csv\n",
      " 4 _ 1 .csv\n",
      " 5 _ 1 .csv\n",
      " 1 _ 2 .csv\n",
      " 1 _ 4 .csv\n",
      " 3 _ 1 .csv\n",
      " 2 _ 1 .csv\n",
      " 2 _ 2 .csv\n",
      " 3 _ 2 .csv\n",
      " 4 _ 4 .csv\n",
      " 5 _ 4 .csv\n",
      " 2 _ 3 .csv\n",
      " 3 _ 3 .csv\n",
      " 1 _ 1 .csv\n",
      " 3 _ 4 .csv\n",
      " 2 _ 4 .csv\n",
      " 5 _ 3 .csv\n",
      " 4 _ 3 .csv\n",
      " 5 _ 2 .csv\n",
      " 4 _ 2 .csv\n",
      "yield_per_plant\n",
      " 1 _ 3 .csv\n",
      " 4 _ 1 .csv\n",
      " 5 _ 1 .csv\n",
      " 1 _ 2 .csv\n",
      " 1 _ 4 .csv\n",
      " 3 _ 1 .csv\n",
      " 2 _ 1 .csv\n",
      " 2 _ 2 .csv\n",
      " 3 _ 2 .csv\n",
      " 4 _ 4 .csv\n",
      " 5 _ 4 .csv\n",
      " 2 _ 3 .csv\n",
      " 3 _ 3 .csv\n",
      " 1 _ 1 .csv\n",
      " 3 _ 4 .csv\n",
      " 2 _ 4 .csv\n",
      " 5 _ 3 .csv\n",
      " 4 _ 3 .csv\n",
      " 5 _ 2 .csv\n",
      " 4 _ 2 .csv\n",
      " 1 _ 3 .csv\n",
      " 4 _ 1 .csv\n",
      " 5 _ 1 .csv\n",
      " 1 _ 2 .csv\n",
      " 1 _ 4 .csv\n",
      " 3 _ 1 .csv\n",
      " 2 _ 1 .csv\n",
      " 2 _ 2 .csv\n",
      " 3 _ 2 .csv\n",
      " 4 _ 4 .csv\n",
      " 2 _ 4 .csv\n",
      " 5 _ 3 .csv\n",
      " 4 _ 3 .csv\n",
      " 5 _ 2 .csv\n",
      " 4 _ 2 .csv\n",
      " 1 _ 3 .csv\n",
      " 4 _ 1 .csv\n",
      " 5 _ 1 .csv\n",
      " 1 _ 2 .csv\n",
      " 1 _ 4 .csv\n",
      " 3 _ 1 .csv\n",
      " 2 _ 1 .csv\n",
      " 2 _ 2 .csv\n",
      " 3 _ 2 .csv\n",
      " 4 _ 4 .csv\n",
      " 5 _ 4 .csv\n",
      " 2 _ 3 .csv\n",
      " 3 _ 3 .csv\n",
      " 1 _ 1 .csv\n",
      " 3 _ 4 .csv\n",
      " 2 _ 4 .csv\n",
      " 5 _ 3 .csv\n",
      " 4 _ 3 .csv\n",
      " 5 _ 2 .csv\n",
      " 4 _ 2 .csv\n",
      " 1 _ 3 .csv\n",
      " 4 _ 1 .csv\n",
      " 5 _ 1 .csv\n",
      " 1 _ 2 .csv\n",
      " 1 _ 4 .csv\n",
      " 3 _ 1 .csv\n",
      " 2 _ 1 .csv\n",
      " 2 _ 2 .csv\n",
      " 3 _ 2 .csv\n",
      " 4 _ 4 .csv\n",
      " 5 _ 4 .csv\n",
      " 2 _ 3 .csv\n",
      " 3 _ 3 .csv\n",
      " 1 _ 1 .csv\n",
      " 3 _ 4 .csv\n",
      " 2 _ 4 .csv\n",
      " 5 _ 3 .csv\n",
      " 4 _ 3 .csv\n",
      " 5 _ 2 .csv\n",
      " 4 _ 2 .csv\n",
      " 1 _ 3 .csv\n",
      " 4 _ 1 .csv\n",
      " 5 _ 1 .csv\n",
      " 1 _ 2 .csv\n",
      " 1 _ 4 .csv\n",
      " 3 _ 1 .csv\n",
      " 2 _ 1 .csv\n",
      " 2 _ 2 .csv\n",
      " 3 _ 2 .csv\n",
      " 4 _ 4 .csv\n",
      " 5 _ 4 .csv\n",
      " 2 _ 3 .csv\n",
      " 3 _ 3 .csv\n",
      " 1 _ 1 .csv\n",
      " 3 _ 4 .csv\n",
      " 2 _ 4 .csv\n",
      " 5 _ 3 .csv\n",
      " 4 _ 3 .csv\n",
      " 5 _ 2 .csv\n",
      " 4 _ 2 .csv\n",
      "no_tuber_scabpit\n",
      " 1 _ 3 .csv\n",
      " 4 _ 1 .csv\n",
      " 5 _ 1 .csv\n",
      " 1 _ 2 .csv\n",
      " 1 _ 4 .csv\n",
      " 3 _ 1 .csv\n",
      " 2 _ 1 .csv\n",
      " 2 _ 2 .csv\n",
      " 3 _ 2 .csv\n",
      " 4 _ 4 .csv\n",
      " 5 _ 4 .csv\n",
      " 2 _ 3 .csv\n",
      " 3 _ 3 .csv\n",
      " 1 _ 1 .csv\n",
      " 3 _ 4 .csv\n",
      " 2 _ 4 .csv\n",
      " 5 _ 3 .csv\n",
      " 4 _ 3 .csv\n",
      " 5 _ 2 .csv\n",
      " 4 _ 2 .csv\n",
      " 1 _ 3 .csv\n",
      " 4 _ 1 .csv\n",
      " 5 _ 1 .csv\n",
      " 1 _ 2 .csv\n",
      " 1 _ 4 .csv\n",
      " 3 _ 1 .csv\n",
      " 2 _ 1 .csv\n"
     ]
    }
   ],
   "source": [
    "path_response='/Users/rosa/Desktop/ALLWork/Madison/Project/Soil-nn/Code/python code local/Main Data Files/response_sklearn/'  \n",
    "for file_response in os.listdir(path_response):  \n",
    "    if (file_response != '.DS_Store') & (file_response != 'Icon\\r'):  \n",
    "        print(file_response)  \n",
    "        path_r= path_response+file_response  \n",
    "        os.chdir(path_r)  \n",
    "        for re in os.listdir(path_r):  \n",
    "            if re[0:8] == 'response':  \n",
    "                response = pd.read_csv(path_response+file_response+'/'+re)  \n",
    "                response.rename(columns={'Column1':'Link_ID','x1':'y_b'}, inplace=True)  \n",
    "                response.drop(columns=response.columns[2], inplace=True)  \n",
    "                response=response.drop(columns='Variety2')  \n",
    "                path_x = '/Users/rosa/Desktop/ALLWork/Madison/Project/Soil-nn/Code/python code local/Main Data Files/normalized_data_sklearn/'  \n",
    "                writer= pd.ExcelWriter(path_r+'/'+'classification_RF_FS_notImportant'+'.xlsx', engine='xlsxwriter')   \n",
    "                for file_folder in os.listdir(path_x):  \n",
    "                    if (file_folder[-4:] != '.csv') & (file_folder != '.DS_Store')& (file_folder != 'Icon\\r'):          \n",
    "                        path = path_x+file_folder  \n",
    "                        os.chdir(path)  \n",
    "                        file_list = []  \n",
    "                        tRF=pd.DataFrame()  \n",
    "                        tcluster=pd.DataFrame()  \n",
    "                        k=0  \n",
    "                        for file in os.listdir(path):  \n",
    "                            if (file[0] != 't') & (file[-4:] == '.csv') & (file != '.DS_Store')& (file_folder != 'Icon\\r'):  \n",
    "                                print(file)  \n",
    "                                file_list.append(file)  \n",
    "                                data_temp = pd.read_csv(file) \n",
    "                                data_temp.rename(columns={'Unnamed: 0':'Link_ID'}, inplace=True) \n",
    "                                A = [item for item in SELECTED_FEATURE_0[file_folder][file_response]]\n",
    "                                A.insert(0, \"Link_ID\")\n",
    "                                data_temp = data_temp[A]\n",
    "                                data_temp.rename(columns={'Unnamed: 0':'Link_ID'}, inplace=True)  \n",
    "                                data=pd.merge(response,data_temp,on='Link_ID')\n",
    "                                \n",
    "                                data.drop(columns = 'Link_ID',inplace=True)  \n",
    "                                data_train,data_val = train_test_split(data,train_size=0.8, random_state=42)  \n",
    "                                output = process_data(data_train,data_val,cv)  \n",
    "                                tRF[k]=pd.DataFrame(output['f1-score'].values)         \n",
    "                                k=k+1  \n",
    "                        tRF.to_excel(writer, sheet_name=file_folder, index=True)  \n",
    "                writer.save()                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a76155",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
