{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "69988add",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "import os\n",
    "from sklearn.ensemble import GradientBoostingClassifier,RandomForestRegressor\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pandas import ExcelWriter\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import openpyxl\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8a7fbaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "33c53725",
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers=[]\n",
    "def detect_outlier(data_1):\n",
    "    \n",
    "    threshold=1.5\n",
    "    mean_1 = np.mean(data_1)\n",
    "    std_1 =np.std(data_1)\n",
    "    \n",
    "    \n",
    "    for y in data_1:\n",
    "        z_score= (y - mean_1)/std_1 \n",
    "        if  z_score > threshold:\n",
    "            outliers.append(y)\n",
    "    return outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ab79a226",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data_train,data_val,cv): \n",
    "    report_All= []\n",
    "    x_column_list = data_train.drop(columns=['y_c']).columns  \n",
    "    #Classification RF  \n",
    "    \n",
    "    pipeRF = Pipeline([('classifier', [RandomForestRegressor()])])  \n",
    "    param_grid = [  \n",
    "    {'classifier' : [RandomForestRegressor()], \n",
    "    'classifier__n_estimators': [100, 200],\n",
    "    'classifier__min_samples_split': [8, 10],\n",
    "    'classifier__min_samples_leaf': [3, 4, 5],\n",
    "    'classifier__max_depth': [80, 90]}]  \n",
    "\n",
    "    \n",
    "    clf = GridSearchCV(pipeRF, param_grid = param_grid, cv = cv, n_jobs=-1, scoring='neg_mean_squared_error')  \n",
    "    # Fit on data  \n",
    "    clf.fit(data_train[x_column_list],data_train['y_c'])  \n",
    "    best_clf=clf.best_estimator_  \n",
    "    y_valid=best_clf.predict(data_val[x_column_list])\n",
    "    \n",
    "    report_All.append(metrics.mean_absolute_error(data_val['y_c'].values, y_valid))\n",
    "    report_All.append(metrics.mean_squared_error(data_val['y_c'].values, y_valid))\n",
    "    report_All.append(metrics.mean_squared_error(data_val['y_c'].values, y_valid))\n",
    "    report_All.append(r2_score(data_val['y_c'].values, y_valid))\n",
    "    #report_All = classification_report(data_val['y_c'],y_valid,output_dict=True) \n",
    "    \n",
    "    dAll=pd.DataFrame(report_All)\n",
    "    return dAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0fc9ebe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv=RepeatedKFold(n_splits=10,n_repeats=3, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726a0441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yield_per_meter\n",
      " 1 _ 3 .csv\n",
      " 4 _ 1 .csv\n",
      " 5 _ 1 .csv\n",
      " 1 _ 2 .csv\n",
      " 1 _ 4 .csv\n",
      " 3 _ 1 .csv\n",
      " 2 _ 1 .csv\n",
      " 2 _ 2 .csv\n",
      " 3 _ 2 .csv\n",
      " 4 _ 4 .csv\n",
      " 5 _ 4 .csv\n",
      " 2 _ 3 .csv\n",
      " 3 _ 3 .csv\n",
      " 1 _ 1 .csv\n",
      " 3 _ 4 .csv\n",
      " 2 _ 4 .csv\n",
      " 5 _ 3 .csv\n",
      " 4 _ 3 .csv\n",
      " 5 _ 2 .csv\n",
      " 4 _ 2 .csv\n",
      " 1 _ 3 .csv\n",
      " 4 _ 1 .csv\n",
      " 5 _ 1 .csv\n",
      " 1 _ 2 .csv\n",
      " 1 _ 4 .csv\n",
      " 3 _ 1 .csv\n",
      " 2 _ 1 .csv\n",
      " 2 _ 2 .csv\n",
      " 3 _ 2 .csv\n",
      " 4 _ 4 .csv\n",
      " 5 _ 4 .csv\n",
      " 2 _ 3 .csv\n",
      " 3 _ 3 .csv\n",
      " 1 _ 1 .csv\n",
      " 3 _ 4 .csv\n",
      " 2 _ 4 .csv\n",
      " 5 _ 3 .csv\n",
      " 4 _ 3 .csv\n",
      " 5 _ 2 .csv\n",
      " 4 _ 2 .csv\n",
      " 1 _ 3 .csv\n",
      " 4 _ 1 .csv\n",
      " 5 _ 1 .csv\n",
      " 1 _ 2 .csv\n",
      " 1 _ 4 .csv\n",
      " 3 _ 1 .csv\n",
      " 2 _ 1 .csv\n",
      " 2 _ 2 .csv\n",
      " 3 _ 2 .csv\n",
      " 4 _ 4 .csv\n",
      " 5 _ 4 .csv\n",
      " 2 _ 3 .csv\n",
      " 3 _ 3 .csv\n",
      " 1 _ 1 .csv\n",
      " 3 _ 4 .csv\n",
      " 2 _ 4 .csv\n",
      " 5 _ 3 .csv\n",
      " 4 _ 3 .csv\n",
      " 5 _ 2 .csv\n",
      " 4 _ 2 .csv\n",
      " 1 _ 3 .csv\n",
      " 4 _ 1 .csv\n",
      " 5 _ 1 .csv\n",
      " 1 _ 2 .csv\n",
      " 1 _ 4 .csv\n",
      " 3 _ 1 .csv\n",
      " 2 _ 1 .csv\n",
      " 2 _ 2 .csv\n",
      " 3 _ 2 .csv\n",
      " 4 _ 4 .csv\n",
      " 5 _ 4 .csv\n",
      " 2 _ 3 .csv\n",
      " 3 _ 3 .csv\n",
      " 1 _ 1 .csv\n",
      " 3 _ 4 .csv\n",
      " 2 _ 4 .csv\n",
      " 5 _ 3 .csv\n",
      " 4 _ 3 .csv\n",
      " 5 _ 2 .csv\n",
      " 4 _ 2 .csv\n",
      " 1 _ 3 .csv\n",
      " 4 _ 1 .csv\n",
      " 5 _ 1 .csv\n",
      " 1 _ 2 .csv\n",
      " 1 _ 4 .csv\n",
      " 3 _ 1 .csv\n",
      " 2 _ 1 .csv\n",
      " 2 _ 2 .csv\n",
      " 3 _ 2 .csv\n",
      " 4 _ 4 .csv\n",
      " 5 _ 4 .csv\n",
      " 2 _ 3 .csv\n",
      " 3 _ 3 .csv\n",
      " 1 _ 1 .csv\n",
      " 3 _ 4 .csv\n",
      " 2 _ 4 .csv\n",
      " 5 _ 3 .csv\n",
      " 4 _ 3 .csv\n",
      " 5 _ 2 .csv\n",
      " 4 _ 2 .csv\n",
      "yield_per_plant\n",
      " 1 _ 3 .csv\n",
      " 4 _ 1 .csv\n",
      " 5 _ 1 .csv\n",
      " 1 _ 2 .csv\n",
      " 1 _ 4 .csv\n",
      " 3 _ 1 .csv\n",
      " 2 _ 1 .csv\n",
      " 2 _ 2 .csv\n",
      " 3 _ 2 .csv\n",
      " 4 _ 4 .csv\n",
      " 5 _ 4 .csv\n",
      " 2 _ 3 .csv\n",
      " 3 _ 3 .csv\n",
      " 1 _ 1 .csv\n",
      " 3 _ 4 .csv\n",
      " 2 _ 4 .csv\n",
      " 5 _ 3 .csv\n",
      " 4 _ 3 .csv\n",
      " 5 _ 2 .csv\n",
      " 4 _ 2 .csv\n",
      " 1 _ 3 .csv\n",
      " 4 _ 1 .csv\n",
      " 5 _ 1 .csv\n",
      " 1 _ 2 .csv\n",
      " 1 _ 4 .csv\n",
      " 3 _ 1 .csv\n",
      " 2 _ 1 .csv\n",
      " 2 _ 2 .csv\n",
      " 3 _ 2 .csv\n",
      " 4 _ 4 .csv\n",
      " 5 _ 4 .csv\n",
      " 2 _ 3 .csv\n",
      " 3 _ 3 .csv\n",
      " 1 _ 1 .csv\n",
      " 3 _ 4 .csv\n",
      " 2 _ 4 .csv\n",
      " 5 _ 3 .csv\n",
      " 4 _ 3 .csv\n",
      " 5 _ 2 .csv\n",
      " 4 _ 2 .csv\n",
      " 1 _ 3 .csv\n",
      " 4 _ 1 .csv\n",
      " 5 _ 1 .csv\n",
      " 1 _ 2 .csv\n",
      " 1 _ 4 .csv\n",
      " 3 _ 1 .csv\n",
      " 2 _ 1 .csv\n"
     ]
    }
   ],
   "source": [
    "path_response='/Users/rosa/Desktop/ALLWork/Madison/Project/Soil-nn/Code/python code local/Main Data Files/yield_response/'  \n",
    "for file_response in os.listdir(path_response):  \n",
    "    if (file_response != '.DS_Store') & (file_response != 'Icon\\r'):  \n",
    "        print(file_response)  \n",
    "        path_r= path_response+file_response  \n",
    "        os.chdir(path_r)  \n",
    "        for re in os.listdir(path_r):  \n",
    "            if re[0:8] == 'response':  \n",
    "                response = pd.read_csv(path_response+file_response+'/'+re)  \n",
    "                response.rename(columns={'Column1':'Link_ID',response.columns[1]:'y_c'}, inplace=True)  \n",
    "                #response.drop(columns=response.columns[2], inplace=True)  \n",
    "                #response=response.drop(columns='Variety2')  \n",
    "                path_x = '/Users/rosa/Desktop/ALLWork/Madison/Project/Soil-nn/Code/python code local/Main Data Files/normalized_data_sklearn/'  \n",
    "                writer= pd.ExcelWriter(path_r+'/'+'classification_RF'+'.xlsx', engine='xlsxwriter')   \n",
    "                for file_folder in os.listdir(path_x):  \n",
    "                    if (file_folder[-4:] != '.csv') & (file_folder != '.DS_Store')& (file_folder != 'Icon\\r'):          \n",
    "                        path = path_x+file_folder  \n",
    "                        os.chdir(path)  \n",
    "                        file_list = []  \n",
    "                        tRF=pd.DataFrame()  \n",
    "                        tcluster=pd.DataFrame()  \n",
    "                        k=0  \n",
    "                        for file in os.listdir(path):  \n",
    "                            if (file[0] != 't') & (file[-4:] == '.csv') & (file != '.DS_Store')& (file_folder != 'Icon\\r'):  \n",
    "                                print(file)  \n",
    "                                file_list.append(file)  \n",
    "                                data_temp = pd.read_csv(file)  \n",
    "                                data_temp.rename(columns={'Unnamed: 0':'Link_ID'}, inplace=True)  \n",
    "                                data=pd.merge(response,data_temp,on='Link_ID')  \n",
    "                                data.drop(columns = 'Link_ID',inplace=True) \n",
    "                                \n",
    "                                OT = pd.DataFrame(detect_outlier(data['y_c']))\n",
    "                                data['y_c'] = np.where(data['y_c'] >OT.min()[0], OT.min()[0],data['y_c'])\n",
    "                                \n",
    "                                data_train,data_val = train_test_split(data,train_size=0.8, random_state=42)  \n",
    "                                output = process_data(data_train,data_val,cv)  \n",
    "                                tRF[k]=output        \n",
    "                                k=k+1  \n",
    "                        tRF.to_excel(writer, sheet_name=file_folder, index=True)  \n",
    "                writer.save()                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b711bcb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28667a45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72c8d02b",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52dfbb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path2 = '/Users/rosa/Desktop/ALLWork/Madison/Project/Soil-nn/Code/python code local/Main Data Files/feature_ML_disease'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b086acb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_list2 = []\n",
    "#reading files in folder response\n",
    "for root, dirs, files in os.walk(data_path2, topdown=False):\n",
    "    for path in dirs:\n",
    "        path_list2.append(path)\n",
    "#reading sheet name       \n",
    "wb = openpyxl.load_workbook(data_path2+'/'+path+'/feature_selection.xlsx')\n",
    "sheet_list = wb.sheetnames\n",
    "\n",
    "results_dic = dict.fromkeys(sheet_list)\n",
    "\n",
    "for sheet_name in results_dic.keys():\n",
    "    temp_df = pd.DataFrame(columns=path_list2, index=range(0,700))\n",
    "    for folder in path_list2:\n",
    "        data_temp = pd.read_excel(data_path2+'/'+folder+'/feature_selection.xlsx', sheet_name=sheet_name)\n",
    "        temp_df[folder].iloc[\n",
    "            range(0, len(data_temp['Unnamed: 0'].values))] = data_temp['Unnamed: 0'].values\n",
    "\n",
    "    results_dic[sheet_name] = temp_df\n",
    "    \n",
    "for key in results_dic.keys():\n",
    "    results_dic[key] = results_dic[key].iloc[range(0,max(30, round(len(data_temp)/3)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc965cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/Users/rosa/Desktop/ALLWork/Madison/Project/Soil-nn/Code/python code local/Main Data Files/feature_netcomi_disease'\n",
    "\n",
    "path_list = []\n",
    "\n",
    "for root, dirs, files in os.walk(data_path, topdown=False):\n",
    "    for path in dirs:\n",
    "        path_list.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe3015a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "list_level = ['Class', 'Family', 'Genus', 'Order', 'Phylum']\n",
    "\n",
    "all_data = dict.fromkeys(dirs)\n",
    "\n",
    "for folder in path_list:\n",
    "    \n",
    "    all_data[folder] = dict.fromkeys(list_level)\n",
    "    temp_path = data_path+'/'+folder\n",
    "    temp_file_list = os.listdir(temp_path)\n",
    "    for file in temp_file_list:\n",
    "        if 'Count' in file:\n",
    "            level = file.split('_')[-1][:-4]\n",
    "            data_temp = pd.read_csv(data_path+'/'+folder+'/'+file,index_col=0)\n",
    "            data_temp.sort_values(by='dif_close', ascending=False, inplace=True)\n",
    "            #all_data[folder][level] = data_temp.iloc[range(0,max(40, round(len(data_temp)/3)))]\n",
    "            all_data[folder][level] = data_temp.iloc[range(0,max(30, round(len(data_temp)/3)))]\n",
    "            #print(level,\":\",max(30 ,round(len(data_temp)/3)))\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b984f302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class\n",
      "76\n",
      "7\n",
      "Family\n",
      "144\n",
      "8\n",
      "Genus\n",
      "247\n",
      "12\n",
      "Order\n",
      "133\n",
      "10\n",
      "Phylum\n",
      "42\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "list_level = ['Class', 'Family', 'Genus', 'Order', 'Phylum']\n",
    "SELECTED_FEATURE = dict.fromkeys(dirs)\n",
    "for col in range(0,len(list_level)):\n",
    "    level = list_level[col]\n",
    "    print(level)\n",
    "    SELECTED_FEATURE[level] = dict.fromkeys(dirs)\n",
    "    response_list = path_list\n",
    "    feature_list = []\n",
    "    for response in response_list:\n",
    "        feature_list.append(all_data[response][level].index.union(results_dic[level][response][results_dic[level][response].notnull()].values))\n",
    "    feature_list = [item for subitem in feature_list for item in subitem]\n",
    "    feature_list = np.unique(feature_list)\n",
    "    print(len(feature_list))\n",
    "    matrix_df = pd.DataFrame(columns = response_list, index = feature_list)\n",
    "    for response in response_list:\n",
    "        for feature in feature_list:\n",
    "                if (feature in all_data[response][level].index) & (feature in results_dic[level][response].values):\n",
    "                    matrix_df[response].loc[feature] = 3\n",
    "                elif(feature in all_data[response][level].index) &(feature not in results_dic[level][response].values):\n",
    "                    matrix_df[response].loc[feature] = 2##NetComi\n",
    "                elif (feature not in all_data[response][level].index) &(feature in results_dic[level][response].values):\n",
    "                    matrix_df[response].loc[feature] = 1##ML\n",
    "                else:\n",
    "                    matrix_df[response].loc[feature] = 0##NotMLnotNetcomi\n",
    "            \n",
    "        #print(matrix_df[response])\n",
    "#         ree=re[matrix_df[response]==3]\n",
    "#         print(ree)\n",
    "#         SELECTED_FEATURE[response][level]= ree.index     \n",
    "        SELECTED_FEATURE[level][response] = matrix_df[response][matrix_df[response]==3].index\n",
    "    print(SELECTED_FEATURE[level][response].shape[0])\n",
    "    matrix_df['Sum'] = 0\n",
    "    for index in matrix_df.index:\n",
    "        matrix_df['Sum'].loc[index] = sum(matrix_df.loc[index].values)\n",
    "    \n",
    "    matrix_df.sort_values(by='Sum', ascending=False, inplace=True)\n",
    "    matrix_df=matrix_df.iloc[1:30,:]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd110bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_tuber_scab\n",
      " 1 _ 3 .csv\n"
     ]
    }
   ],
   "source": [
    "for file_response in os.listdir(path_response):  \n",
    "    if (file_response != '.DS_Store') & (file_response != 'Icon\\r'):  \n",
    "        print(file_response)  \n",
    "        path_r= path_response+file_response  \n",
    "        os.chdir(path_r)  \n",
    "        for re in os.listdir(path_r):  \n",
    "            if re[0:8] == 'response':  \n",
    "                response = pd.read_csv(path_response+file_response+'/'+re)  \n",
    "                response.rename(columns={'Column1':'Link_ID','x1':'y_b'}, inplace=True)  \n",
    "                #response.drop(columns=response.columns[2], inplace=True)  \n",
    "                #response=response.drop(columns='Variety2')  \n",
    "                path_x = '/Users/rosa/Desktop/ALLWork/Madison/Project/Soil-nn/Code/python code local/Main Data Files/normalized_data_sklearn/'  \n",
    "                writer= pd.ExcelWriter(path_r+'/'+'classification_RF_FS'+'.xlsx', engine='xlsxwriter')   \n",
    "                for file_folder in os.listdir(path_x):  \n",
    "                    if (file_folder[-4:] != '.csv') & (file_folder != '.DS_Store')& (file_folder != 'Icon\\r'):          \n",
    "                        path = path_x+file_folder  \n",
    "                        os.chdir(path)  \n",
    "                        file_list = []  \n",
    "                        tRF=pd.DataFrame()  \n",
    "                        tcluster=pd.DataFrame()  \n",
    "                        k=0  \n",
    "                        for file in os.listdir(path):  \n",
    "                            if (file[0] != 't') & (file[-4:] == '.csv') & (file != '.DS_Store')& (file_folder != 'Icon\\r'):  \n",
    "                                print(file)  \n",
    "                                file_list.append(file)  \n",
    "                                data_temp = pd.read_csv(file) \n",
    "                                data_temp.rename(columns={'Unnamed: 0':'Link_ID'}, inplace=True) \n",
    "                                A = [item for item in SELECTED_FEATURE[file_folder][file_response]]\n",
    "                                A.insert(0, \"Link_ID\")\n",
    "                                data_temp = data_temp[A]\n",
    "                                data_temp.rename(columns={'Unnamed: 0':'Link_ID'}, inplace=True)  \n",
    "                                data=pd.merge(response,data_temp,on='Link_ID')\n",
    "                                \n",
    "                                data.drop(columns = 'Link_ID',inplace=True)  \n",
    "                                data_train,data_val = train_test_split(data,train_size=0.8, random_state=42)  \n",
    "                                output = process_data(data_train,data_val,cv)  \n",
    "                                tRF[k]=pd.DataFrame(output['f1-score'].values)         \n",
    "                                k=k+1  \n",
    "                        tRF.to_excel(writer, sheet_name=file_folder, index=True)  \n",
    "                writer.save()                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d977064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'no_tuber_scab': None,\n",
       " 'no_tuber_scabpit': None,\n",
       " 'no_tuber_scabsuper': None,\n",
       " 'Class': {'no_tuber_scab': Index(['Gitt.GS.136', 'Holophagae', 'Ktedonobacteria', 'Microgenomatia',\n",
       "         'Negativicutes', 'Nitrospiria', 'OLB14', 'Parcubacteria',\n",
       "         'S0134.terrestrial.group', 'Saccharimonadia', 'Subgroup.22'],\n",
       "        dtype='object'),\n",
       "  'no_tuber_scabpit': Index(['Gemmatimonadetes', 'Gitt.GS.136', 'Holophagae', 'JG30.KF.CM66',\n",
       "         'Ktedonobacteria', 'OLB14', 'Oligoflexia', 'Planctomycetes',\n",
       "         'Polyangia', 'Saccharimonadia'],\n",
       "        dtype='object'),\n",
       "  'no_tuber_scabsuper': Index(['Gammaproteobacteria', 'Ktedonobacteria', 'Myxococcia', 'Phycisphaerae',\n",
       "         'Polyangia', 'S0134.terrestrial.group', 'Subgroup.22'],\n",
       "        dtype='object')},\n",
       " 'Family': {'no_tuber_scab': Index(['Koribacteraceae', 'LWQ8', 'Lachnospiraceae', 'Microbacteriaceae',\n",
       "         'Micrococcaceae', 'Nitrospiraceae', 'Planococcaceae', 'Rhizobiaceae',\n",
       "         'Rhizobiales.Incertae.Sedis', 'Rubritaleaceae', 'SC.I.84',\n",
       "         'Solibacteraceae', 'Streptomycetaceae'],\n",
       "        dtype='object'),\n",
       "  'no_tuber_scabpit': Index(['Gemmatimonadaceae', 'Kineosporiaceae', 'Ktedonobacteraceae', 'LWQ8',\n",
       "         'Micrococcaceae', 'Solibacteraceae', 'Sporichthyaceae'],\n",
       "        dtype='object'),\n",
       "  'no_tuber_scabsuper': Index(['Gemmatimonadaceae', 'Ilumatobacteraceae', 'Ktedonobacteraceae', 'LWQ8',\n",
       "         'Methylococcaceae', 'Phycisphaeraceae', 'Planococcaceae',\n",
       "         'Pseudonocardiaceae'],\n",
       "        dtype='object')},\n",
       " 'Genus': {'no_tuber_scab': Index(['Luteimonas', 'Luteolibacter', 'Marmoricola', 'Microbacterium',\n",
       "         'Microvirga', 'Pedobacter', 'Pseudolabrys', 'RB41', 'Rhodoplanes',\n",
       "         'Roseiarcus', 'Solirubrobacter', 'Subgroup.10'],\n",
       "        dtype='object'),\n",
       "  'no_tuber_scabpit': Index(['Kineosporia', 'Microvirga', 'Ohtaekwangia', 'Pedomicrobium',\n",
       "         'Phaselicystis', 'Pseudoxanthomonas', 'Rhodoplanes', 'Solirubrobacter',\n",
       "         'Sorangium', 'Steroidobacter'],\n",
       "        dtype='object'),\n",
       "  'no_tuber_scabsuper': Index(['Fictibacillus', 'Iamia', 'Jatrophihabitans', 'KD3.10', 'Massilia',\n",
       "         'Microlunatus', 'Pedococcus.Phycicoccus', 'Pedomicrobium',\n",
       "         'Rhodoplanes', 'Roseiarcus', 'SM1A02', 'Sporosarcina'],\n",
       "        dtype='object')},\n",
       " 'Order': {'no_tuber_scab': Index(['Elsterales', 'Erysipelotrichales',\n",
       "         'Gammaproteobacteria.Incertae.Sedis', 'Gemmatales', 'Isosphaerales',\n",
       "         'Nitrospirales', 'Pedosphaerales', 'Solibacterales',\n",
       "         'Sphingobacteriales'],\n",
       "        dtype='object'),\n",
       "  'no_tuber_scabpit': Index(['Elsterales', 'Kineosporiales', 'Pedosphaerales', 'Rhizobiales',\n",
       "         'Rickettsiales', 'SBR1031', 'Sphingomonadales', 'Subgroup.7'],\n",
       "        dtype='object'),\n",
       "  'no_tuber_scabsuper': Index(['Isosphaerales', 'Micropepsales', 'Paenibacillales', 'Pedosphaerales',\n",
       "         'Pseudonocardiales', 'Pyrinomonadales', 'RBG.13.54.9', 'Rhizobiales',\n",
       "         'Rickettsiales', 'Solibacterales'],\n",
       "        dtype='object')},\n",
       " 'Phylum': {'no_tuber_scab': Index(['Acidobacteriota', 'Actinobacteriota', 'Bacteroidota',\n",
       "         'Bdellovibrionota', 'Chloroflexi', 'Cyanobacteria', 'Elusimicrobiota',\n",
       "         'Entotheonellaeota', 'Firmicutes', 'Gemmatimonadota', 'Halobacterota',\n",
       "         'NB1.j', 'Patescibacteria', 'Planctomycetota', 'Proteobacteria',\n",
       "         'RCP2.54', 'SAR324.clade.Marine.group.B.', 'Spirochaetota',\n",
       "         'Sumerlaeota', 'Verrucomicrobiota', 'WPS.2'],\n",
       "        dtype='object'),\n",
       "  'no_tuber_scabpit': Index(['Acidobacteriota', 'Actinobacteriota', 'Bacteroidota',\n",
       "         'Bdellovibrionota', 'Cyanobacteria', 'Desulfobacterota',\n",
       "         'Elusimicrobiota', 'Firmicutes', 'Gemmatimonadota', 'NB1.j',\n",
       "         'Nitrospirota', 'Patescibacteria', 'Planctomycetota', 'Proteobacteria',\n",
       "         'RCP2.54', 'Spirochaetota', 'Thermoplasmatota', 'Verrucomicrobiota',\n",
       "         'WPS.2', 'WS2'],\n",
       "        dtype='object'),\n",
       "  'no_tuber_scabsuper': Index(['Acidobacteriota', 'Actinobacteriota', 'Armatimonadota', 'Bacteroidota',\n",
       "         'Bdellovibrionota', 'Chloroflexi', 'Crenarchaeota', 'Cyanobacteria',\n",
       "         'Entotheonellaeota', 'Fibrobacterota', 'Firmicutes', 'Gemmatimonadota',\n",
       "         'Halobacterota', 'NB1.j', 'Nanoarchaeota', 'Nitrospirota',\n",
       "         'Patescibacteria', 'Planctomycetota', 'Proteobacteria', 'RCP2.54',\n",
       "         'WPS.2'],\n",
       "        dtype='object')}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " SELECTED_FEATURE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65988628",
   "metadata": {},
   "source": [
    "# Feature_selection with score 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad970875",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_level = ['Class', 'Family', 'Genus', 'Order', 'Phylum']\n",
    "SELECTED_FEATURE = dict.fromkeys(dirs)\n",
    "SELECTED_FEATURE_0 = dict.fromkeys(dirs)\n",
    "for col in range(0,len(list_level)):\n",
    "    level = list_level[col]\n",
    "    print(level)\n",
    "    SELECTED_FEATURE[level] = dict.fromkeys(dirs)\n",
    "    SELECTED_FEATURE_0[level] = dict.fromkeys(dirs)\n",
    "    response_list = path_list\n",
    "    feature_list = []\n",
    "    for response in response_list:\n",
    "        feature_list.append(all_data[response][level].index.union(results_dic[level][response][results_dic[level][response].notnull()].values))\n",
    "    feature_list = [item for subitem in feature_list for item in subitem]\n",
    "    feature_list = np.unique(feature_list)\n",
    "    print(len(feature_list))\n",
    "    matrix_df = pd.DataFrame(columns = response_list, index = feature_list)\n",
    "    for response in response_list:\n",
    "        for feature in feature_list:\n",
    "                if (feature in all_data[response][level].index) & (feature in results_dic[level][response].values):\n",
    "                    matrix_df[response].loc[feature] = 3\n",
    "                elif(feature in all_data[response][level].index) &(feature not in results_dic[level][response].values):\n",
    "                    matrix_df[response].loc[feature] = 2##NetComi\n",
    "                elif (feature not in all_data[response][level].index) &(feature in results_dic[level][response].values):\n",
    "                    matrix_df[response].loc[feature] = 1##ML\n",
    "                else:\n",
    "                    matrix_df[response].loc[feature] = 0##NotMLnotNetcomi\n",
    "            \n",
    "        #print(matrix_df[response])\n",
    "#         ree=re[matrix_df[response]==3]\n",
    "#         print(ree)\n",
    "#         SELECTED_FEATURE[response][level]= ree.index     \n",
    "        SELECTED_FEATURE[level][response] = matrix_df[response][matrix_df[response]==3].index\n",
    "        print(SELECTED_FEATURE[level][response].shape[0])\n",
    "        SELECTED_FEATURE_0[level][response] = matrix_df[response][matrix_df[response]==0].index[0:SELECTED_FEATURE[level][response].shape[0]]\n",
    "        #print(SELECTED_FEATURE_0[level][response].shape[0])\n",
    "    \n",
    "    matrix_df['Sum'] = 0\n",
    "    for index in matrix_df.index:\n",
    "        matrix_df['Sum'].loc[index] = sum(matrix_df.loc[index].values)\n",
    "    \n",
    "    matrix_df.sort_values(by='Sum', ascending=False, inplace=True)\n",
    "    matrix_df=matrix_df.iloc[1:30,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f89a7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_response='/Users/rosa/Desktop/ALLWork/Madison/Project/Soil-nn/Code/python code local/Main Data Files/response_sklearn/'  \n",
    "for file_response in os.listdir(path_response):  \n",
    "    if (file_response != '.DS_Store') & (file_response != 'Icon\\r'):  \n",
    "        print(file_response)  \n",
    "        path_r= path_response+file_response  \n",
    "        os.chdir(path_r)  \n",
    "        for re in os.listdir(path_r):  \n",
    "            if re[0:8] == 'response':  \n",
    "                response = pd.read_csv(path_response+file_response+'/'+re)  \n",
    "                response.rename(columns={'Column1':'Link_ID','x1':'y_b'}, inplace=True)  \n",
    "                response.drop(columns=response.columns[2], inplace=True)  \n",
    "                response=response.drop(columns='Variety2')  \n",
    "                path_x = '/Users/rosa/Desktop/ALLWork/Madison/Project/Soil-nn/Code/python code local/Main Data Files/normalized_data_sklearn/'  \n",
    "                writer= pd.ExcelWriter(path_r+'/'+'classification_RF_FS_notImportant'+'.xlsx', engine='xlsxwriter')   \n",
    "                for file_folder in os.listdir(path_x):  \n",
    "                    if (file_folder[-4:] != '.csv') & (file_folder != '.DS_Store')& (file_folder != 'Icon\\r'):          \n",
    "                        path = path_x+file_folder  \n",
    "                        os.chdir(path)  \n",
    "                        file_list = []  \n",
    "                        tRF=pd.DataFrame()  \n",
    "                        tcluster=pd.DataFrame()  \n",
    "                        k=0  \n",
    "                        for file in os.listdir(path):  \n",
    "                            if (file[0] != 't') & (file[-4:] == '.csv') & (file != '.DS_Store')& (file_folder != 'Icon\\r'):  \n",
    "                                print(file)  \n",
    "                                file_list.append(file)  \n",
    "                                data_temp = pd.read_csv(file) \n",
    "                                data_temp.rename(columns={'Unnamed: 0':'Link_ID'}, inplace=True) \n",
    "                                A = [item for item in SELECTED_FEATURE_0[file_folder][file_response]]\n",
    "                                A.insert(0, \"Link_ID\")\n",
    "                                data_temp = data_temp[A]\n",
    "                                data_temp.rename(columns={'Unnamed: 0':'Link_ID'}, inplace=True)  \n",
    "                                data=pd.merge(response,data_temp,on='Link_ID')\n",
    "                                \n",
    "                                data.drop(columns = 'Link_ID',inplace=True)  \n",
    "                                data_train,data_val = train_test_split(data,train_size=0.8, random_state=42)  \n",
    "                                output = process_data(data_train,data_val,cv)  \n",
    "                                tRF[k]=pd.DataFrame(output['f1-score'].values)         \n",
    "                                k=k+1  \n",
    "                        tRF.to_excel(writer, sheet_name=file_folder, index=True)  \n",
    "                writer.save()                      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
