{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e85111e",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux, Statistics\n",
    "using Flux: onehotbatch, onecold, crossentropy, throttle, params \n",
    "using Base.Iterators: repeated, partition\n",
    "using Printf, BSON\n",
    "using CSV\n",
    "using DataFrames\n",
    "using Tables\n",
    "\n",
    "##########################################################################################\n",
    "#                                                                                        #\n",
    "#                                  BRIEF INTRODUCTION                                    #\n",
    "#                                                                                        #\n",
    "##########################################################################################\n",
    "# In this model, we use an Autoencoder to compress the information in the OTUs to        #\n",
    "# a lower dimension.                                                                     #\n",
    "##########################################################################################\n",
    "# Currently the target dimension is 50, but this number is due to change in the future.  #\n",
    "##########################################################################################\n",
    "# The AE implemented in this notebook is the UnderComplete AutoEncoder. The reduced      #\n",
    "# dimension is self-defined and the model forced the compression to the target dimension.#\n",
    "# If overfitting occurred (which is not the case in dimension 50 and 100), I would use   #\n",
    "# De-noising AutoEncoder instead, which is the same model but its output layer has random#\n",
    "# noise added to it.                                                                     #\n",
    "##########################################################################################\n",
    "# If time permits, I would also try the sparse AutoEncoder, which adds a regularizer to  #\n",
    "# the loss function and introduces sparsity. In this model, I would be able to let the   #\n",
    "# model decide what is the optimal dimension for all necessary information.              #\n",
    "##########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "442446a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "make_fold (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#=\n",
    "This function add the data into each fold for K-fold Cross-Validation.\n",
    "INPUT:\n",
    "data: the whole un-partitioned dataset\n",
    "idx : the index of the data that should be included into this fold\n",
    "RETURN:\n",
    "A tuple that includes the inputs(data) and outputs(label) of this fold.\n",
    "In this case, the data and the labels are identical\n",
    "=#\n",
    "function make_fold(data, idx)\n",
    "    # The 2D array for data in each folder.The dimension is 22*2394\n",
    "    data_batch = Array{Float32, 2}(undef, length(idx), length(data[1,:]))\n",
    "    # Add all data for this folder into the batch\n",
    "    for i in 1:length(idx)\n",
    "        data_batch[i,:] = data[idx[i],:]\n",
    "    end\n",
    "    return (data_batch', data_batch')\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41d6fe17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "k_fold_partition (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#=\n",
    "This function partition the whole dataset into 10 folds\n",
    "INPUT:\n",
    "otu_batch: the whole dataset\n",
    "RETURN:\n",
    "the whole dataset divided into 10 folds\n",
    "=#\n",
    "function k_fold_partition(otu_batch)\n",
    "    # partition the whole dataset into 10 folds\n",
    "    fold_idx = partition(1:length(otu_batch[:,1]), length(otu_batch[:,1])÷10+1)\n",
    "    # call make_fold and store the 10 folds\n",
    "    whole_set = [make_fold(otu_batch, i) for i in fold_idx]\n",
    "\n",
    "    return whole_set\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c8806ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  Chain(\n",
       "    Dense(2394 => 800, σ),              \u001b[90m# 1_916_000 parameters\u001b[39m\n",
       "    Dense(800 => 200, σ),               \u001b[90m# 160_200 parameters\u001b[39m\n",
       "    Dense(200 => 50),                   \u001b[90m# 10_050 parameters\u001b[39m\n",
       "  ),\n",
       "  Chain(\n",
       "    Dense(50 => 200, σ),                \u001b[90m# 10_200 parameters\u001b[39m\n",
       "    Dense(200 => 800, σ),               \u001b[90m# 160_800 parameters\u001b[39m\n",
       "    Dense(800 => 2394, σ),              \u001b[90m# 1_917_594 parameters\u001b[39m\n",
       "  ),\n",
       ") \u001b[90m                  # Total: 12 arrays, \u001b[39m4_174_844 parameters, 15.927 MiB."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#=\n",
    "This is the model for the UnderComplete AutoEncoder.\n",
    "It uses 2 layers for encoding and decoding instead of one.\n",
    "The advantage is that it compress the information in two steps instead of one \n",
    "radical step, which in theory would be more stable.\n",
    "The disadvantage is that the computation is much slower.\n",
    "The code layer is of dimension 50.\n",
    "The activation functions are sigmoid function. It is recommanded more by online \n",
    "sources than Relu.\n",
    "=#\n",
    "model = Chain(\n",
    "    Chain(\n",
    "    Dense(2394, 800, σ),\n",
    "    Dense(800, 200, σ),\n",
    "    Dense(200, 50)),\n",
    "    Chain(\n",
    "    Dense(50, 200, σ),\n",
    "    Dense(200, 800, σ),\n",
    "    Dense(800, 2394, σ))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "04997aff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train (generic function with 1 method)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#=\n",
    "This function train the 10 folds and validate\n",
    "The procedure for each of the fold is as follows:\n",
    "1. set 1 fold as the validation set and the rest as training set\n",
    "2. reset all parameters in the model\n",
    "3. train the model for at max 200 epoches\n",
    "4. for each epoch, calculate the loss for the validation set\n",
    "5. if the loss increases, increment the counter, otherwise set the loss as best loss\n",
    "6. if the loss for validation set has increased for 5 continuous epoches, terminate \n",
    "   to prevent overfitting\n",
    "7. return the best loss for this folder\n",
    "=#\n",
    "function train(whole_set) \n",
    "    uni_best = 1\n",
    "    # loop through all 10 folders\n",
    "    for k in 1:10 \n",
    "        println(\"Start training on the \", k, \"th fold...\")\n",
    "        # set the training set and the testing set\n",
    "        train_set = whole_set[Not(k)]\n",
    "        test_set = whole_set[k]\n",
    "        # reset all the parameters\n",
    "        Flux.loadparams!(model, map(p -> p .= randn.(), Flux.params(model)))\n",
    "        # here the loss function is MSE, I also tried cross-entropy. \n",
    "        # I'll write down the result for both in the conclusion section\n",
    "        loss(x, y) = Flux.Losses.mse(model(x),y)\n",
    "        # record the number of continuous epoches that the loss increases\n",
    "        loss_inc = 0\n",
    "        # the loss of the current epoch for the validation set\n",
    "        val_loss = 1000\n",
    "        # the lowest loss so far in this fold\n",
    "        best_loss = 1000\n",
    "\n",
    "        # the optimizer is Adam with learning rate of 0.001\n",
    "        opt = ADAM(0.001)\n",
    "        # the maximum epoch is 200\n",
    "        for epoch_idx in 1:300\n",
    "            # train the network\n",
    "            Flux.train!(loss, params(model), train_set, opt)\n",
    "            # calculate the validation loss for this epoch\n",
    "            val_loss = loss(test_set...)\n",
    "            # if the loss increases, increment the counter\n",
    "            if val_loss >= best_loss \n",
    "                loss_inc += 1\n",
    "            else\n",
    "                # if not, then set the current loss as lowest\n",
    "                best_loss = val_loss\n",
    "                loss_inc = 0\n",
    "                if best_loss < uni_best\n",
    "                    uni_best = best_loss\n",
    "                    BSON.@save \"AENN.bson\" model epoch_idx uni_best\n",
    "                end\n",
    "            end\n",
    "            # loss has increased for 5 continuous epoch, exit to prevent overfitting\n",
    "            if loss_inc >= 5 && opt.eta > 1e-6\n",
    "                opt.eta /= 10.0\n",
    "                loss_inc = 0\n",
    "            end\n",
    "            \n",
    "            if loss_inc >= 10\n",
    "                println(\"Force exit to prevent overfit at epoch: \", epoch_idx)\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "        println(\"Finished training the \", k, \"th folder...\" )\n",
    "        println(\"The final validation loss is: \", best_loss)\n",
    "        println(\"------------------------------------\")\n",
    "    end\n",
    "    return uni_best\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b6bd03c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The untrained loss for each folder is: 0.46488762, 0.4618976, 0.46499193, 0.46676403, 0.4609492, 0.46118698, 0.4636053, 0.46428007, 0.46435606, 0.46840236, Start training...\n",
      "Start training on the 1th fold...\n",
      "Force exit to prevent overfit at epoch: 126\n",
      "Finished training the 1th folder...\n",
      "The final validation loss is: 0.14770779\n",
      "------------------------------------\n",
      "Start training on the 2th fold...\n",
      "Force exit to prevent overfit at epoch: 98\n",
      "Finished training the 2th folder...\n",
      "The final validation loss is: 0.15950239\n",
      "------------------------------------\n",
      "Start training on the 3th fold...\n",
      "Force exit to prevent overfit at epoch: 171\n",
      "Finished training the 3th folder...\n",
      "The final validation loss is: 0.15170877\n",
      "------------------------------------\n",
      "Start training on the 4th fold...\n",
      "Finished training the 4th folder...\n",
      "The final validation loss is: 0.13265596\n",
      "------------------------------------\n",
      "Start training on the 5th fold...\n",
      "Finished training the 5th folder...\n",
      "The final validation loss is: 0.13689606\n",
      "------------------------------------\n",
      "Start training on the 6th fold...\n",
      "Force exit to prevent overfit at epoch: 174\n",
      "Finished training the 6th folder...\n",
      "The final validation loss is: 0.14198384\n",
      "------------------------------------\n",
      "Start training on the 7th fold...\n",
      "Finished training the 7th folder...\n",
      "The final validation loss is: 0.12886404\n",
      "------------------------------------\n",
      "Start training on the 8th fold...\n",
      "Force exit to prevent overfit at epoch: 153\n",
      "Finished training the 8th folder...\n",
      "The final validation loss is: 0.14661738\n",
      "------------------------------------\n",
      "Start training on the 9th fold...\n",
      "Force exit to prevent overfit at epoch: 253\n",
      "Finished training the 9th folder...\n",
      "The final validation loss is: 0.13803041\n",
      "------------------------------------\n",
      "Start training on the 10th fold...\n",
      "Force exit to prevent overfit at epoch: 198\n",
      "Finished training the 10th folder...\n",
      "The final validation loss is: 0.14120413\n",
      "------------------------------------\n",
      "The lowest loss among all models is: 0.12886404\n"
     ]
    }
   ],
   "source": [
    "#=\n",
    "This is the wrapper function to perform the training\n",
    "=#\n",
    "\n",
    "# Load data from the CSV file and transform them into an array\n",
    "data = CSV.read(\"../processed-data/otu-yield-per-plant.csv\", DataFrame)\n",
    "data_arr = Matrix(data)\n",
    "# only select the OTUs\n",
    "otu = data_arr[:, 2:2395]\n",
    "# partition them into 10 folds\n",
    "whole_set = k_fold_partition(otu)\n",
    "\n",
    "# reset the parameter of the model to get the untrained loss, just as a reference\n",
    "Flux.loadparams!(model, map(p -> p .= randn.(), Flux.params(model)))\n",
    "loss(x, y) = Flux.Losses.mse(model(x),y)\n",
    "print(\"The untrained loss for each folder is: \")\n",
    "for i in 1:10\n",
    "    print(loss(whole_set[i]...), \", \")\n",
    "end\n",
    "\n",
    "# Start the training\n",
    "println(\"Start training...\")\n",
    "best_loss = train(whole_set)\n",
    "println(\"The lowest loss among all models is: \", best_loss)\n",
    "\n",
    "##########################################################################################\n",
    "#                                                                                        #\n",
    "#                                      CONCLUSION                                        #\n",
    "#                                                                                        #\n",
    "##########################################################################################\n",
    "# Not much problem has occured during the training process. The overfitting did not      #\n",
    "# occured. However, whether the model is good enough to compress the necessary           #\n",
    "# information is to be discussed. More on this later.                                    #\n",
    "##########################################################################################\n",
    "# For the model itself, I tried both 100 and 50 as target dimension. Based on the result,#\n",
    "# 100 and 50 have similar validation loss, which indicates that 50 encoded neurons has   #\n",
    "# as much information as 100 encoded neurons. This is good.                              #\n",
    "##########################################################################################\n",
    "# I tried MSE and cross-entropy as loss functions.                                       #\n",
    "# For Cross-entropy:                                                                     #\n",
    "# The loss for untrained model is around 20, and the trained model has                   #\n",
    "# loss between 0.5 to 1.5, which some outliers with 3 or 5 losses. I found out that the  #\n",
    "# loss converge towards 1.5 rapidly in the first 3 epoches for most cases. However, it   #\n",
    "# is known that Cross-entropy is good for classification, and MSE is better for          #\n",
    "# regression. Here we have a regression problem.                                         #\n",
    "# For MSE:                                                                               #\n",
    "# The loss for untrained model is much smaller, around 0.47 to 0.48. After the training, #\n",
    "# the loss decreased to 0.13 to 0.16, and is very stable with no outlier.                #\n",
    "##########################################################################################\n",
    "# The big question for now is how to analyse the quality of the model.                   #\n",
    "# For classification problems, the universal method is accuracy by comparing the one-hot #\n",
    "# encoded output. This cannot be done in this case, as accuracy would be 0 (having       #\n",
    "# exactly the same value for input and output is very bad).                              #\n",
    "# The method I'm using now is to check the loss. Indeed the loss decreased a lot, but I  #\n",
    "# do not know if it is good enough, as there is not a reference for comparison. One way  #\n",
    "# is to normalize all the value so that the loss in more universal in some way.          #\n",
    "##########################################################################################\n",
    "# Next step for AutoEncoder:                                                             #\n",
    "# I will put the code layer into a feedforward NN, just to compare its performance       #\n",
    "# against the random selection model. If time permits, I want to implement the variants  #\n",
    "# I mentioned in the beginning of this notebook. However, I do want to focus on the BNN  #\n",
    "# as that is the main challenge. Thus, if the performance is better in the feedforward   #\n",
    "# NN, then I would move forward to BNN, and maybe visit this part later in the project.  #\n",
    "##########################################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.3",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
