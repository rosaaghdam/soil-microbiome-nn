{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e85111e",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux, Statistics\n",
    "using Flux: onehotbatch, onecold, crossentropy, throttle\n",
    "using Base.Iterators: repeated, partition\n",
    "using Printf, BSON\n",
    "using CSV\n",
    "using DataFrames\n",
    "using Tables\n",
    "\n",
    "##########################################################################################\n",
    "#                                                                                        #\n",
    "#                                  BRIEF INTRODUCTION                                    #\n",
    "#                                                                                        #\n",
    "##########################################################################################\n",
    "# In this model, we use an Autoencoder to compress the information in the OTUs to        #\n",
    "# a lower dimension.                                                                     #\n",
    "##########################################################################################\n",
    "# Currently the target dimension is 50, but this number is due to change in the future.  #\n",
    "##########################################################################################\n",
    "# The AE implemented in this notebook is the UnderComplete AutoEncoder. The reduced      #\n",
    "# dimension is self-defined and the model forced the compression to the target dimension.#\n",
    "# If overfitting occurred (which is not the case in dimension 50 and 100), I would use   #\n",
    "# De-noising AutoEncoder instead, which is the same model but its output layer has random#\n",
    "# noise added to it.                                                                     #\n",
    "##########################################################################################\n",
    "# If time permits, I would also try the sparse AutoEncoder, which adds a regularizer to  #\n",
    "# the loss function and introduces sparsity. In this model, I would be able to let the   #\n",
    "# model decide what is the optimal dimension for all necessary information.              #\n",
    "##########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "442446a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "make_fold (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#=\n",
    "This function add the data into each fold for K-fold Cross-Validation.\n",
    "INPUT:\n",
    "data: the whole un-partitioned dataset\n",
    "idx : the index of the data that should be included into this fold\n",
    "RETURN:\n",
    "A tuple that includes the inputs(data) and outputs(label) of this fold.\n",
    "In this case, the data and the labels are identical\n",
    "=#\n",
    "function make_fold(data, idx)\n",
    "    # The 2D array for data in each folder.The dimension is 22*2394\n",
    "    data_batch = Array{Float32, 2}(undef, length(idx), length(data[1,:]))\n",
    "    # Add all data for this folder into the batch\n",
    "    for i in 1:length(idx)\n",
    "        data_batch[i,:] = data[idx[i],:]\n",
    "    end\n",
    "    return (data_batch', data_batch')\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41d6fe17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "k_fold_partition (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#=\n",
    "This function partition the whole dataset into 10 folds\n",
    "INPUT:\n",
    "otu_batch: the whole dataset\n",
    "RETURN:\n",
    "the whole dataset divided into 10 folds\n",
    "=#\n",
    "function k_fold_partition(otu_batch)\n",
    "    # partition the whole dataset into 10 folds\n",
    "    fold_idx = partition(1:length(otu_batch[:,1]), length(otu_batch[:,1])÷10+1)\n",
    "    # call make_fold and store the 10 folds\n",
    "    whole_set = [make_fold(otu_batch, i) for i in fold_idx]\n",
    "\n",
    "    return whole_set\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c8806ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(Chain(Dense(2394, 800, σ), Dense(800, 200, σ), Dense(200, 50)), Chain(Dense(50, 200, σ), Dense(200, 800, σ), Dense(800, 2394, σ)))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#=\n",
    "This is the model for the UnderComplete AutoEncoder.\n",
    "It uses 2 layers for encoding and decoding instead of one.\n",
    "The advantage is that it compress the information in two steps instead of one \n",
    "radical step, which in theory would be more stable.\n",
    "The disadvantage is that the computation is much slower.\n",
    "The code layer is of dimension 50.\n",
    "The activation functions are sigmoid function. It is recommanded more by online \n",
    "sources than Relu.\n",
    "=#\n",
    "model = Chain(\n",
    "    Chain(\n",
    "    Dense(2394, 800, σ),\n",
    "    Dense(800, 200, σ),\n",
    "    Dense(200, 50)),\n",
    "    Chain(\n",
    "    Dense(50, 200, σ),\n",
    "    Dense(200, 800, σ),\n",
    "    Dense(800, 2394, σ))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04997aff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function train(whole_set) \n",
    "    for k in 1:10 \n",
    "        println(\"Start training on the \", k, \"th fold...\")\n",
    "        train_set = gpu(whole_set[Not(k)])\n",
    "        test_set = gpu(whole_set[k])\n",
    "        # reset all the parameters\n",
    "        Flux.loadparams!(model, map(p -> p .= randn.(), Flux.params(model)))\n",
    "        \n",
    "        loss(x, y) = crossentropy(model(x),y)\n",
    "        loss_inc = 0\n",
    "        val_loss = 1000\n",
    "        best_loss = 1000\n",
    "        last_val_loss = 1000\n",
    "        \n",
    "        opt = ADAM(0.001)\n",
    "        \n",
    "        for epoch_idx in 1:200\n",
    "            Flux.train!(loss, params(model), train_set, opt)\n",
    "            val_loss = loss(test_set...)\n",
    "            if val_loss >= last_val_loss \n",
    "                loss_inc += 1\n",
    "            else\n",
    "                best_loss = val_loss\n",
    "            end\n",
    "            last_val_loss = val_loss\n",
    "            if loss_inc >= 5\n",
    "                println(\"Overfitting, force quit at epoch \", epoch_idx)\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "        println(\"Finished training the \", k, \"th folder...\" )\n",
    "        println(\"The final validation loss is: \", best_loss)\n",
    "        println(\"------------------------------------\")\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6bd03c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The untrained loss for each folder is: 20.07151, 19.86875, 21.555265, 22.037834, 22.264091, 23.456688, 23.189516, 24.2721, 24.593622, 23.640228, Start training...\n",
      "Start training on the 1th fold...\n",
      "Overfitting, force quit at epoch 85\n",
      "Finished training the 1th folder...\n",
      "The final validation loss is: 1.6548467\n",
      "------------------------------------\n",
      "Start training on the 2th fold...\n",
      "Finished training the 2th folder...\n",
      "The final validation loss is: 1.1548356\n",
      "------------------------------------\n",
      "Start training on the 3th fold...\n",
      "Overfitting, force quit at epoch 185\n",
      "Finished training the 3th folder...\n",
      "The final validation loss is: 1.1771184\n",
      "------------------------------------\n",
      "Start training on the 4th fold...\n",
      "Overfitting, force quit at epoch 110\n",
      "Finished training the 4th folder...\n",
      "The final validation loss is: 1.7424244\n",
      "------------------------------------\n",
      "Start training on the 5th fold...\n",
      "Overfitting, force quit at epoch 114\n",
      "Finished training the 5th folder...\n",
      "The final validation loss is: 0.8311132\n",
      "------------------------------------\n",
      "Start training on the 6th fold...\n",
      "Overfitting, force quit at epoch 126\n",
      "Finished training the 6th folder...\n",
      "The final validation loss is: 0.236307\n",
      "------------------------------------\n",
      "Start training on the 7th fold...\n",
      "Overfitting, force quit at epoch 172\n",
      "Finished training the 7th folder...\n",
      "The final validation loss is: 0.5711394\n",
      "------------------------------------\n",
      "Start training on the 8th fold...\n",
      "Overfitting, force quit at epoch 99\n",
      "Finished training the 8th folder...\n",
      "The final validation loss is: 3.8667574\n",
      "------------------------------------\n",
      "Start training on the 9th fold...\n",
      "Overfitting, force quit at epoch 103\n",
      "Finished training the 9th folder...\n",
      "The final validation loss is: 0.9717442\n",
      "------------------------------------\n",
      "Start training on the 10th fold...\n",
      "Overfitting, force quit at epoch 134\n",
      "Finished training the 10th folder...\n",
      "The final validation loss is: 0.9763031\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# This cell load data and change them into the data format I want\n",
    "# It then partition the data into\n",
    "\n",
    "# Load data from the CSV file\n",
    "data = CSV.read(\"../preliminary-model/processed-data/otu-yield-per-plant.csv\", DataFrame)\n",
    "data_arr = Matrix(data)\n",
    "otu = data_arr[:, 2:2395]\n",
    "whole_set = k_fold_partition(otu)\n",
    "\n",
    "Flux.loadparams!(model, map(p -> p .= randn.(), Flux.params(model)))\n",
    "loss(x, y) = crossentropy(model(x),y)\n",
    "\n",
    "print(\"The untrained loss for each folder is: \")\n",
    "for i in 1:10\n",
    "    print(loss(whole_set[i]...), \", \")\n",
    "end\n",
    "\n",
    "println(\"Start training...\")\n",
    "train(whole_set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.1",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
