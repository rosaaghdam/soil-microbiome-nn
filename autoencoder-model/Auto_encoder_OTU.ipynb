{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8e85111e",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux, Statistics\n",
    "using Flux: onehotbatch, onecold, crossentropy, throttle\n",
    "using Base.Iterators: repeated, partition\n",
    "using Printf, BSON\n",
    "using CSV\n",
    "using DataFrames\n",
    "using Tables\n",
    "\n",
    "# In this model, we use an Autoencoder to compress the information in the OTUs to\n",
    "# a lower dimension. The target dimension is 100, which is comparable to the preliminary\n",
    "# model. I would also expect to try both undercomplete and denoising AE. If time permits,\n",
    "# I also want to try sparse AE, however this will be in another notebook as the model\n",
    "# would be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "442446a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "make_fold (generic function with 1 method)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This function divide the data into 10 part and combine the otu with labels\n",
    "# and return them in a batch\n",
    "# So each batch has 22 tuples of 50 otus and an encoded label\n",
    "function make_fold(data, idx)\n",
    "    # batch for otu, 100*22\n",
    "    data_batch = Array{Float32, 2}(undef, length(idx), length(data[1,:]))\n",
    "    for i in 1:length(idx)\n",
    "        data_batch[i,:] = data[idx[i],:]\n",
    "    end\n",
    "    return (data_batch', data_batch')\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "41d6fe17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "k_fold_partition (generic function with 1 method)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since we have very small amount of dataset,I am going to use 10-fold validation here\n",
    "function k_fold_partition(otu_batch)\n",
    "    # partition the whole dataset into 10 folds\n",
    "    fold_idx = partition(1:length(otu_batch[:,1]), length(otu_batch[:,1])÷10+1)\n",
    "    # call make_fold and store the 10 folds\n",
    "    whole_set = [make_fold(otu_batch, i) for i in fold_idx]\n",
    "\n",
    "    return whole_set\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0c8806ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(Chain(Dense(2394, 1000, σ), Dense(1000, 400, σ), Dense(400, 100)), Chain(Dense(100, 400, σ), Dense(400, 1000, σ), Dense(1000, 2394, σ)))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Chain(\n",
    "    Chain(\n",
    "    Dense(2394, 1000, σ),\n",
    "    Dense(1000, 400, σ),\n",
    "    Dense(400, 100)),\n",
    "    Chain(\n",
    "    Dense(100, 400, σ),\n",
    "    Dense(400, 1000, σ),\n",
    "    Dense(1000, 2394, σ))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "04997aff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train (generic function with 1 method)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function train(whole_set) \n",
    "    for k in 1:10 \n",
    "        println(\"Start training on the \", k, \"th fold...\")\n",
    "        train_set = gpu(whole_set[Not(k)])\n",
    "        test_set = gpu(whole_set[k])\n",
    "        # reset all the parameters\n",
    "        Flux.loadparams!(model, map(p -> p .= randn.(), Flux.params(model)))\n",
    "        \n",
    "        loss(x, y) = crossentropy(model(x),y)\n",
    "        loss_inc = 0\n",
    "        val_loss = 0\n",
    "        last_val_loss = 100\n",
    "        \n",
    "        opt = ADAM(0.001)\n",
    "        \n",
    "        for epoch_idx in 1:100\n",
    "            Flux.train!(loss, params(model), train_set, opt)\n",
    "            val_loss = loss(test_set...)\n",
    "            if val_loss > last_val_loss \n",
    "                loss_inc += 1\n",
    "            end\n",
    "            last_val_loss = val_loss\n",
    "            if loss_inc >= 3\n",
    "                println(\"Overfitting, force quit at epoch \", epoch_idx)\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "        println(\"Finished training the \", k, \"th folder...\" )\n",
    "        println(\"The final validation loss is: \", last_val_loss)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b6bd03c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The untrained loss for each folder is: 18.289469, 19.105185, 19.363474, 18.959044, 20.85761, 19.659395, 20.16257, 19.77652, 19.373173, 18.625883, Start training...\n",
      "Start training on the 1th fold...\n",
      "Finished training the 1th folder...\n",
      "The final validation loss is: 0.98374456\n",
      "Start training on the 2th fold...\n",
      "Overfitting, force quit at epoch 87\n",
      "Finished training the 2th folder...\n",
      "The final validation loss is: 1.3892647\n",
      "Start training on the 3th fold...\n",
      "Overfitting, force quit at epoch 51\n",
      "Finished training the 3th folder...\n",
      "The final validation loss is: 1.2635081\n",
      "Start training on the 4th fold...\n",
      "Finished training the 4th folder...\n",
      "The final validation loss is: 3.2856605\n",
      "Start training on the 5th fold...\n",
      "Finished training the 5th folder...\n",
      "The final validation loss is: 0.5024388\n",
      "Start training on the 6th fold...\n",
      "Overfitting, force quit at epoch 96\n",
      "Finished training the 6th folder...\n",
      "The final validation loss is: 5.8994045\n",
      "Start training on the 7th fold...\n",
      "Finished training the 7th folder...\n",
      "The final validation loss is: 1.195452\n",
      "Start training on the 8th fold...\n",
      "Overfitting, force quit at epoch 73\n",
      "Finished training the 8th folder...\n",
      "The final validation loss is: 1.7734802\n",
      "Start training on the 9th fold...\n",
      "Finished training the 9th folder...\n",
      "The final validation loss is: 2.7467523\n",
      "Start training on the 10th fold...\n",
      "Overfitting, force quit at epoch 82\n",
      "Finished training the 10th folder...\n",
      "The final validation loss is: 0.909665\n"
     ]
    }
   ],
   "source": [
    "# This cell load data and change them into the data format I want\n",
    "# It then partition the data into\n",
    "\n",
    "# Load data from the CSV file\n",
    "data = CSV.read(\"../preliminary-model/processed-data/otu-yield-per-plant.csv\", DataFrame)\n",
    "data_arr = Matrix(data)\n",
    "otu = data_arr[:, 2:2395]\n",
    "whole_set = k_fold_partition(otu)\n",
    "\n",
    "Flux.loadparams!(model, map(p -> p .= randn.(), Flux.params(model)))\n",
    "loss(x, y) = crossentropy(model(x),y)\n",
    "\n",
    "print(\"The untrained loss for each folder is: \")\n",
    "for i in 1:10\n",
    "    print(loss(whole_set[i]...), \", \")\n",
    "end\n",
    "\n",
    "println(\"Start training...\")\n",
    "train(whole_set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.1",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
