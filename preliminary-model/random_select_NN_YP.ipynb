{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f03f7a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux, Statistics\n",
    "using Flux: onehotbatch, onecold, crossentropy, throttle\n",
    "using Base.Iterators: repeated, partition\n",
    "using Printf, BSON\n",
    "using CSV\n",
    "using DataFrames\n",
    "\n",
    "# In this model, we test how randomly select OTUs would work in the same model. \n",
    "# This model will only work for yield_per_plant as outcome, but I would assume that\n",
    "# it will also work for others with minor modification.\n",
    "# I will randomly select 100 out of 2395 OTUs as predictors and feed it into the NN\n",
    "# For the hidden layer, I would only add one hidden layer. As discussed in many websites\n",
    "# I viewed, under very rare circumstances would a second layer improve the performance.\n",
    "# Also, with 2 or more layer, the NN would be harder to train, and it is bad with the \n",
    "# small amount of data we have.\n",
    "# For the number of neurons, I read about a general rule of thumb that \"the optimal size \n",
    "# of the hidden layer is between the input size and the output size:, so I set the hidden \n",
    "# size to 52.\n",
    "# For output, I split the values of yield_per_plant into 4 categories, based on quantile. \n",
    "# This might be problematic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baeda7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Loading data from the .csv file\n",
      "└ @ Main In[2]:4\n",
      "┌ Info: Data Processing...\n",
      "└ @ Main In[2]:9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of all otu values: 0.0013581864052314813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Loading Dataset...Done\n",
      "└ @ Main In[2]:46\n"
     ]
    }
   ],
   "source": [
    "##### This cell import the processed data for yield_per_plant and transform the outcome data.\n",
    "# It also transform the data into array type so it's easier to work with.\n",
    "\n",
    "@info(\"Loading data from the .csv file\")\n",
    "# Load data from the CSV file\n",
    "data = CSV.read(\"./processed-data/otu-yield-per-plant.csv\", DataFrame)\n",
    "data_arr = Matrix(data)\n",
    "\n",
    "@info(\"Data Processing...\")\n",
    "# Split the array into otu and label\n",
    "otu = data_arr[:, 2:2395]\n",
    "label = data_arr[:, 2396]\n",
    "# Transform the array into 4 categories based on quantile\n",
    "for i in 1:length(label)\n",
    "    if label[i] <= 751\n",
    "        label[i] = 0\n",
    "    elseif label[i] <= 1068\n",
    "        label[i] = 1\n",
    "    elseif label[i] <= 1444\n",
    "        label[i] = 2\n",
    "    else\n",
    "        label[i] = 3\n",
    "    end\n",
    "end\n",
    "\n",
    "# transform the label into onehot encoding\n",
    "# label_batch = onehotbatch(label, 0:3)\n",
    "\n",
    "# select 50 unique numbers between 1 to 2394 (the 50 unique predictor)\n",
    "# if not unique, select again\n",
    "rand_num = zeros(Int, 100)\n",
    "while true\n",
    "    rand_num = rand((1:2394),100)\n",
    "    if length(unique(rand_num)) == 100\n",
    "        break\n",
    "    end\n",
    "end\n",
    "\n",
    "# select the corresponding OTUs\n",
    "otu_batch = otu[:,rand_num]\n",
    "\n",
    "a = Flux.Data.DataLoader(otu_batch, batchsize=22)\n",
    "\n",
    "println(\"Mean of all otu values: \", mean(otu_batch))\n",
    "\n",
    "@info(\"Loading Dataset...Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48e1a2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Feed-Forward NN construction\n",
      "└ @ Main In[3]:2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Chain(Dense(100, 52, relu), Dense(52, 4), softmax)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model construction\n",
    "@info(\"Feed-Forward NN construction\")\n",
    "model = Chain(\n",
    "    # Input 100 predictors and feed into 52 neurons in the hidden layer\n",
    "    # use ReLu as activation function\n",
    "    Dense(100, 52, relu),\n",
    "    # feed 27 neurons to the output, which consists of 4 categories\n",
    "    Dense(52, 4),\n",
    "    # use softmax as the activation function\n",
    "    softmax\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e39846df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Dividing data into 10 folds...\n",
      "└ @ Main In[4]:3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking folded data...\n",
      "Number of folds:10\n",
      "Type of each fold:Tuple{LinearAlgebra.Adjoint{Float32,Array{Float32,2}},Flux.OneHotMatrix{Array{Flux.OneHotVector,1}}}\n",
      "Number of data entries each fold contains:22\n",
      "Number of predictors for each entry:100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: 10-folder split, Done...\n",
      "└ @ Main In[4]:27\n"
     ]
    }
   ],
   "source": [
    "# Since we have very small amount of dataset,I am going to use 10-fold validation here\n",
    "\n",
    "@info(\"Dividing data into 10 folds...\")\n",
    "# This function divide the data into 10 part and combine the otu with labels\n",
    "# and return them in a batch\n",
    "# So each batch has 22 tuples of 50 otus and an encoded label\n",
    "function make_fold(data, label, idx)\n",
    "    # batch for otu, 50*22\n",
    "    data_batch = Array{Float32, 2}(undef, length(idx), length(otu_batch[1,:]))\n",
    "    for i in 1:length(idx)\n",
    "        data_batch[i,:] = data[idx[i],:]\n",
    "    end\n",
    "    # batch for label, 1(onehot encoding)*22\n",
    "    label_batch = onehotbatch(label[idx], 0:3)\n",
    "    return (data_batch', label_batch)\n",
    "end\n",
    "\n",
    "# partition the whole dataset into 10 folds\n",
    "fold_idx = partition(1:length(otu_batch[:,1]), length(otu_batch[:,1])÷10+1)\n",
    "# call make_fold and store the 10 folds\n",
    "whole_set = [make_fold(otu_batch, label, i) for i in fold_idx]\n",
    "println(\"Checking folded data...\")\n",
    "println(\"Number of folds:\",length(whole_set))\n",
    "println(\"Type of each fold:\", typeof(whole_set[1]))\n",
    "println(\"Number of data entries each fold contains:\", length(whole_set[1][1][1,:]))\n",
    "println(\"Number of predictors for each entry:\", length(whole_set[1][1][:,1]))\n",
    "@info(\"10-folder split, Done...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38945927",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Beginning training loop...\n",
      "└ @ Main In[5]:23\n",
      "┌ Warning: Current Folder Terminate, best accuracy is 0.22727272727272727\n",
      "└ @ Main In[5]:54\n",
      "┌ Info: Beginning training loop...\n",
      "└ @ Main In[5]:23\n",
      "┌ Warning: Current Folder Terminate, best accuracy is 0.5909090909090909\n",
      "└ @ Main In[5]:54\n",
      "┌ Info: Beginning training loop...\n",
      "└ @ Main In[5]:23\n",
      "┌ Warning: Current Folder Terminate, best accuracy is 0.5454545454545454\n",
      "└ @ Main In[5]:54\n",
      "┌ Info: Beginning training loop...\n",
      "└ @ Main In[5]:23\n",
      "┌ Warning: Current Folder Terminate, best accuracy is 0.5\n",
      "└ @ Main In[5]:54\n",
      "┌ Info: Beginning training loop...\n",
      "└ @ Main In[5]:23\n",
      "┌ Warning: Current Folder Terminate, best accuracy is 0.7727272727272727\n",
      "└ @ Main In[5]:54\n",
      "┌ Info: Beginning training loop...\n",
      "└ @ Main In[5]:23\n",
      "┌ Warning: Current Folder Terminate, best accuracy is 0.7727272727272727\n",
      "└ @ Main In[5]:54\n",
      "┌ Info: Beginning training loop...\n",
      "└ @ Main In[5]:23\n",
      "┌ Warning: Current Folder Terminate, best accuracy is 0.5454545454545454\n",
      "└ @ Main In[5]:54\n",
      "┌ Info: Beginning training loop...\n",
      "└ @ Main In[5]:23\n",
      "┌ Warning: Current Folder Terminate, best accuracy is 0.5454545454545454\n",
      "└ @ Main In[5]:54\n",
      "┌ Info: Beginning training loop...\n",
      "└ @ Main In[5]:23\n",
      "┌ Warning: Current Folder Terminate, best accuracy is 0.8181818181818182\n",
      "└ @ Main In[5]:54\n",
      "┌ Info: Beginning training loop...\n",
      "└ @ Main In[5]:23\n",
      "┌ Warning: Current Folder Terminate, best accuracy is 0.6111111111111112\n",
      "└ @ Main In[5]:54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy for 10 folder:0.5929293\n"
     ]
    }
   ],
   "source": [
    "# Train the model in this cell\n",
    "\n",
    "# record the accuracy for each folder\n",
    "mean_accuracy = zeros(Float32,10)\n",
    "# loop for 10 folds\n",
    "for k in 1:10\n",
    "    # set the kth folder as testing set\n",
    "    train_set = gpu(whole_set[Not(k)])\n",
    "    test_set = gpu(whole_set[k])\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    # set to terminate the epoches if not improved for too long\n",
    "    last_improvement = 0\n",
    "    # crossentropy as loss function\n",
    "    loss(x, y) = crossentropy(model(x),y)\n",
    "    # take mean of accuracy\n",
    "    accuracy(x, y) = mean(onecold(model(x)) .== onecold(y))\n",
    "    # when learning rate is larger(0.1), the accuracy is far worse and some time result in the\n",
    "    # same value with different input\n",
    "    # when it's smaller(0.001), the result does not differ much\n",
    "    opt = ADAM(0.01)\n",
    "    \n",
    "    @info(\"Beginning training loop...\")\n",
    "\n",
    "    # 100 epoches for each folder\n",
    "    for epoch_idx in 1:100\n",
    "        Flux.train!(loss, params(model), train_set, opt)\n",
    "        acc = accuracy(test_set...)\n",
    "\n",
    "        if acc >= 0.999\n",
    "            best_acc = acc\n",
    "            @info(\"Reached our target accuracy of 99.9%...terminate.\")\n",
    "            break\n",
    "        end\n",
    "        # update the best accuracy and last improvement\n",
    "        if acc >= best_acc\n",
    "            best_acc = acc\n",
    "            last_improvement = epoch_idx\n",
    "        end\n",
    "        # no improvement for too long\n",
    "        if epoch_idx - last_improvement >= 15 && opt.eta > 1e-6\n",
    "            opt.eta /= 10.0\n",
    "            # After dropping learning rate, give it a few epochs to improve\n",
    "            last_improvement = epoch_idx\n",
    "        end\n",
    "        if epoch_idx - last_improvement >= 30\n",
    "            @warn(\"Terminate before end of epoches\")\n",
    "            break\n",
    "        end\n",
    "    \n",
    "    end\n",
    "    # save the best accuracy for this folder\n",
    "    mean_accuracy[k] = best_acc\n",
    "    @warn(\"Current Folder Terminate, best accuracy is $(best_acc)\")\n",
    "end\n",
    "# output the average accuracy for 10 folders\n",
    "println(\"Average accuracy for 10 folder:\",mean(mean_accuracy))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.1",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
