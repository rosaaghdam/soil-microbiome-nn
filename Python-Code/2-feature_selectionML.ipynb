{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af999b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.feature_selection import SelectKBest,mutual_info_regression,RFE, f_classif,mutual_info_classif\n",
    "from sklearn.linear_model import  LassoCV,LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor,GradientBoostingClassifier,RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV,train_test_split,RepeatedKFold\n",
    "import os\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84d588f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv=RepeatedKFold(n_splits=10,n_repeats=3, random_state=100)\n",
    "q=0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "815f989d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87f83526",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_local='/Users/rosa/Desktop/ALLWork/Madison/Project/Soil-nn/Code/python code local/Git_RF/'\n",
    "path_x = path_local+'OTU/OTUData-1-1/'  \n",
    "path_response=path_local+'response/response_original/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e512479b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stepwise_selection(data, target,SL_in=0.05,SL_out = 0.05):\n",
    "    initial_features = data.columns.tolist()\n",
    "    best_features = []\n",
    "    while (len(initial_features)>0):\n",
    "        remaining_features = list(set(initial_features)-set(best_features))\n",
    "        new_pval = pd.Series(index=remaining_features)\n",
    "        for new_column in remaining_features:\n",
    "            model = sm.OLS(target, sm.add_constant(data[best_features+[new_column]])).fit()\n",
    "            new_pval[new_column] = model.pvalues[new_column]\n",
    "        min_p_value = new_pval.min()\n",
    "        if(min_p_value<SL_in):\n",
    "            best_features.append(new_pval.idxmin())\n",
    "            while(len(best_features)>0):\n",
    "                best_features_with_constant = sm.add_constant(data[best_features])\n",
    "                p_values = sm.OLS(target, best_features_with_constant).fit().pvalues[1:]\n",
    "                max_p_value = p_values.max()\n",
    "                if(max_p_value >= SL_out):\n",
    "                    excluded_feature = p_values.idxmax()\n",
    "                    best_features.remove(excluded_feature)\n",
    "                else:\n",
    "                    break \n",
    "        else:\n",
    "            break\n",
    "    return best_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2223a6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data,data_train,data_val,cv,q):  \n",
    "    x_column_list = data_train.drop(columns=['y_c']).columns \n",
    "    feature_list1 = data[x_column_list].max().sort_values(ascending=False)[data[x_column_list].max()>np.quantile(data[x_column_list].max(),q)].index\n",
    "    #apply SelectKBest class to extract top 20 best features\n",
    "    bestfeatures = SelectKBest(score_func=mutual_info_regression, k=round(data.shape[1]*(1-q)))\n",
    "    fit = bestfeatures.fit(data[x_column_list],data['y_c'])\n",
    "    dfscores = pd.DataFrame(fit.scores_)\n",
    "    dfcolumns = pd.DataFrame(x_column_list)\n",
    "    #concat two dataframes for better visualization \n",
    "    featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "    featureScores.columns = ['Variable','Score']  #naming the dataframe columns\n",
    "    feature_list2 = featureScores.nlargest(round(data.shape[1]*(1-q)),'Score')\n",
    "    \n",
    "    \n",
    "    feature_list3=stepwise_selection(data[x_column_list],data['y_c'])\n",
    "    reg = LassoCV(cv=cv).fit(data_train[x_column_list], data_train['y_c'])\n",
    "    lassoCoefs0 = pd.DataFrame(\n",
    "    data=reg.coef_[np.where(reg.coef_ != 0)[0]], \n",
    "    index=data[x_column_list].columns[np.where(reg.coef_ != 0)[0]],columns=['LASSO Coefs1'])\n",
    "    feature_list4=lassoCoefs0\n",
    "    \n",
    "    parameters = {'n_estimators':(100, 500),\n",
    "                  'min_samples_split':(3,4,5),\n",
    "                  'min_samples_leaf':(3,4,5)}\n",
    "\n",
    "    gb_model = GradientBoostingRegressor(random_state=7, warm_start=False)\n",
    "    grid_obj = GridSearchCV(gb_model, param_grid=parameters, verbose=1, n_jobs=4, cv=cv)\n",
    "    grid_obj = grid_obj.fit(data_train[x_column_list],data_train['y_c'])\n",
    "    gb_model_best = grid_obj.best_estimator_\n",
    "    y_hat = gb_model_best.predict(data_val[x_column_list])\n",
    "    FeatImportance = gb_model_best.feature_importances_\n",
    "    GBCoefs = pd.DataFrame(index=data_train[x_column_list].columns, data=FeatImportance,columns=['Coefs'])\n",
    "    imp_coef = GBCoefs.sort_values(by='Coefs')\n",
    "    feature_list6=imp_coef.loc[imp_coef['Coefs']>np.quantile(imp_coef,q)]\n",
    "    feature_list6 = [item for item in feature_list6.index]\n",
    "    parameters = {'n_estimators':(10,20,100, 500),\n",
    "              'min_samples_split':(2,3,4),\n",
    "              'min_samples_leaf':(1,2,3)}\n",
    "    rf_model = RandomForestRegressor(warm_start=False)\n",
    "    grid_obj = GridSearchCV(rf_model, param_grid=parameters, verbose=1, n_jobs=4, cv=cv)\n",
    "    grid_obj = grid_obj.fit(data_train[x_column_list],data_train['y_c'])\n",
    "    rf_model_best = grid_obj.best_estimator_\n",
    "    y_hat_rf = rf_model_best.predict(data_val[x_column_list])  \n",
    "    rf_model_best.score(data_val[x_column_list],data_val['y_c'])\n",
    "    FeatImportance = rf_model_best.feature_importances_\n",
    "    RFCoefs = pd.DataFrame(index=data_train[x_column_list].columns, data=FeatImportance,columns=['Coefs'])\n",
    "    imp_coef_RF = RFCoefs.sort_values(by='Coefs')\n",
    "    feature_list7=imp_coef_RF.loc[imp_coef_RF['Coefs']>np.quantile(imp_coef_RF,q)]\n",
    "    feature_list7 = [item for item in feature_list7.index]\n",
    "    \n",
    "    mi = mutual_info_regression(data_train[x_column_list],data_train['y_c'], discrete_features=False, n_neighbors=3, copy=True, random_state=None)\n",
    "    MI=pd.DataFrame(mi)\n",
    "    MI['OTU']=data[x_column_list].columns\n",
    "    MI=MI.set_index('OTU')\n",
    "    MI=MI.sort_values(0,ascending=False)\n",
    "    feature_list8=MI[MI[0]>np.quantile(mi,q)].index\n",
    "    \n",
    "    methodList = ['Maximum', \n",
    "              'KBest', \n",
    "              'Stepwise Regression', \n",
    "              'Lasso CV', \n",
    "              'GBM', \n",
    "              'Random Forest', \n",
    "              'Mutual Info']\n",
    "    featureList = [[item for item in feature_list1], \n",
    "               [item for item in feature_list2['Variable'].values], \n",
    "               feature_list3, \n",
    "               [item for item in feature_list4.index], \n",
    "               feature_list6, \n",
    "               feature_list7,\n",
    "               [item for item in feature_list8]]\n",
    "    featureUniqueList = np.unique([item for sublist in featureList for item in sublist])\n",
    "    featureDictionary = dict.fromkeys(featureUniqueList)\n",
    "    for key in featureDictionary.keys():\n",
    "        featureDictionary[key] = []\n",
    "    \n",
    "\n",
    "    for feature in featureUniqueList:\n",
    "    \n",
    "        for i, method in enumerate(methodList):\n",
    "        \n",
    "            if feature in featureList[i]:\n",
    "                featureDictionary[feature].append(method)   \n",
    "\n",
    "    featureDf = pd.DataFrame(index=featureUniqueList, columns=methodList)\n",
    "    featureDf['Count'] = 0\n",
    "\n",
    "    for feature in featureDictionary.keys(): \n",
    "        for method in methodList:\n",
    "        \n",
    "            if method in featureDictionary[feature]:\n",
    "            \n",
    "                featureDf['Count'][feature] += 1\n",
    "            \n",
    "                featureDf[method][feature] = 'X'\n",
    "            else:\n",
    "                featureDf[method][feature] = '-' \n",
    "    featureDf.sort_values(by='Count', ascending=False, inplace=True)\n",
    "    return featureDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a3081bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_binary(data,data_train,data_val,cv,q):  \n",
    "    x_column_list = data.drop(columns=['y_b']).columns \n",
    "    feature_list1 = data[x_column_list].max().sort_values(ascending=False)[data[x_column_list].max()>np.quantile(data[x_column_list].max(),q)].index\n",
    "    #apply SelectKBest class to extract top  best features\n",
    "    bestfeatures = SelectKBest(score_func=f_classif, k=round(data.shape[1]*(1-q)))\n",
    "    fit = bestfeatures.fit(data[x_column_list],data['y_b'])\n",
    "    dfscores = pd.DataFrame(fit.scores_)\n",
    "    dfcolumns = pd.DataFrame(x_column_list)\n",
    "    #concat two dataframes for better visualization \n",
    "    featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "    featureScores.columns = ['Variable','Score']  #naming the dataframe columns\n",
    "    feature_list2 = featureScores.nlargest(round(data.shape[1]*(1-q)),'Score')\n",
    "    \n",
    "    #Recursive Feature Elimination LogisticRegression\n",
    "    model = LogisticRegression(solver='lbfgs')\n",
    "    rfe = RFE(model, n_features_to_select = round(data.shape[1]*(1-q)))\n",
    "    fit = rfe.fit(data[x_column_list], data['y_b'])\n",
    "    feature_list3 = data[x_column_list].columns[fit.ranking_==1]\n",
    "\n",
    "    #Recursive Feature DecisionTree\n",
    "    model = DecisionTreeClassifier()\n",
    "    rfe = RFE(model, n_features_to_select = round(data.shape[1]*(1-q)))\n",
    "    fit = rfe.fit(data[x_column_list], data['y_b'])\n",
    "    feature_list4 = data[x_column_list].columns[fit.ranking_==1]\n",
    "    \n",
    "    #GradientBoostingClassifier\n",
    "    parameters = {'n_estimators':(100, 500),\n",
    "              'min_samples_split':(3,4),\n",
    "              'min_samples_leaf':(3,4,5)}\n",
    "\n",
    "    gb_model = GradientBoostingClassifier(random_state=7, warm_start=False)\n",
    "    grid_obj = GridSearchCV(gb_model, param_grid=parameters, verbose=1, n_jobs=4, cv=cv)\n",
    "    grid_obj = grid_obj.fit(data_train[x_column_list],data_train['y_b'])\n",
    "    gb_model_best = grid_obj.best_estimator_\n",
    "    FeatImportance = gb_model_best.feature_importances_\n",
    "    GBCoefs = pd.DataFrame(index=data_train[x_column_list].columns, data=FeatImportance,columns=['Coefs'])\n",
    "    imp_coef = GBCoefs.sort_values(by='Coefs')\n",
    "    feature_list6=imp_coef.loc[imp_coef['Coefs']>np.quantile(imp_coef,q)]\n",
    "    feature_list6 = [item for item in feature_list6.index]\n",
    "\n",
    "    # RandomForestClassifier\n",
    "    parameters = {'n_estimators':(10,20,100, 500),\n",
    "          'min_samples_split':(2,3,4),\n",
    "          'min_samples_leaf':(1,2,3)}\n",
    "    rf_model = RandomForestClassifier(warm_start=False)\n",
    "    grid_obj = GridSearchCV(rf_model, param_grid=parameters, verbose=1, n_jobs=-1, cv=cv)\n",
    "    grid_obj = grid_obj.fit(data_train[x_column_list],data_train['y_b'])\n",
    "    rf_model_best = grid_obj.best_estimator_\n",
    "    y_hat_rf = rf_model_best.predict(data_val[x_column_list])  \n",
    "    FeatImportance = rf_model_best.feature_importances_\n",
    "    RFCoefs = pd.DataFrame(index=data_train[x_column_list].columns, data=FeatImportance,columns=['Coefs'])\n",
    "    imp_coef_RF = RFCoefs.sort_values(by='Coefs')\n",
    "    feature_list7=imp_coef_RF.loc[imp_coef_RF['Coefs']>np.quantile(imp_coef_RF,q)]\n",
    "    feature_list7 = [item for item in feature_list7.index]\n",
    "    #mutual_info_classif\n",
    "    mi = mutual_info_classif(data_train[x_column_list],data_train['y_b'], discrete_features=False, n_neighbors=3, copy=True, random_state=None)\n",
    "    MI=pd.DataFrame(mi)\n",
    "    MI['OTU']=data[x_column_list].columns\n",
    "    MI=MI.set_index('OTU')\n",
    "    MI=MI.sort_values(0,ascending=False)\n",
    "    feature_list8=MI[MI[0]>np.quantile(mi,q)].index\n",
    "    \n",
    "    methodList = ['Maximum', \n",
    "              'KBest', \n",
    "              'RFE_logistic', \n",
    "              'RFE_RF', \n",
    "              'GBM', \n",
    "              'Random Forest', \n",
    "              'Mutual Info']\n",
    "    featureList = [[item for item in feature_list1], \n",
    "               [item for item in feature_list2['Variable'].values], \n",
    "               feature_list3, \n",
    "               [item for item in feature_list4], \n",
    "               feature_list6, \n",
    "               feature_list7,\n",
    "               [item for item in feature_list8]]\n",
    "    featureUniqueList = np.unique([item for sublist in featureList for item in sublist])\n",
    "    featureDictionary = dict.fromkeys(featureUniqueList)\n",
    "    for key in featureDictionary.keys():\n",
    "        featureDictionary[key] = []\n",
    "    \n",
    "\n",
    "    for feature in featureUniqueList:\n",
    "    \n",
    "        for i, method in enumerate(methodList):\n",
    "        \n",
    "            if feature in featureList[i]:\n",
    "                featureDictionary[feature].append(method)   \n",
    "\n",
    "    featureDf = pd.DataFrame(index=featureUniqueList, columns=methodList)\n",
    "    featureDf['Count'] = 0\n",
    "\n",
    "    for feature in featureDictionary.keys(): \n",
    "        for method in methodList:\n",
    "        \n",
    "            if method in featureDictionary[feature]:\n",
    "            \n",
    "                featureDf['Count'][feature] += 1\n",
    "            \n",
    "                featureDf[method][feature] = 'X'\n",
    "            else:\n",
    "                featureDf[method][feature] = '-' \n",
    "    featureDf.sort_values(by='Count', ascending=False, inplace=True)\n",
    "    return featureDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "584b03d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7f0fd60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yield_Plant\n",
      " 1 _ 1 .csv\n",
      "Fitting 30 folds for each of 18 candidates, totalling 540 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m                 data\u001b[38;5;241m.\u001b[39mdrop(columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLink_ID\u001b[39m\u001b[38;5;124m'\u001b[39m,inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \n\u001b[1;32m     28\u001b[0m                 data_train,data_val \u001b[38;5;241m=\u001b[39m train_test_split(data,train_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m) \n\u001b[0;32m---> 29\u001b[0m                 output \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdata_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdata_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\u001b[43mq\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     30\u001b[0m         output\u001b[38;5;241m.\u001b[39mto_excel(writer, sheet_name\u001b[38;5;241m=\u001b[39mfile_folder, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)    \n\u001b[1;32m     31\u001b[0m writer\u001b[38;5;241m.\u001b[39msave()\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mprocess_data\u001b[0;34m(data, data_train, data_val, cv, q)\u001b[0m\n\u001b[1;32m     26\u001b[0m gb_model \u001b[38;5;241m=\u001b[39m GradientBoostingRegressor(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m, warm_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     27\u001b[0m grid_obj \u001b[38;5;241m=\u001b[39m GridSearchCV(gb_model, param_grid\u001b[38;5;241m=\u001b[39mparameters, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, cv\u001b[38;5;241m=\u001b[39mcv)\n\u001b[0;32m---> 28\u001b[0m grid_obj \u001b[38;5;241m=\u001b[39m \u001b[43mgrid_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx_column_list\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdata_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43my_c\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m gb_model_best \u001b[38;5;241m=\u001b[39m grid_obj\u001b[38;5;241m.\u001b[39mbest_estimator_\n\u001b[1;32m     30\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m gb_model_best\u001b[38;5;241m.\u001b[39mpredict(data_val[x_column_list])\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/soil_env2/lib/python3.10/site-packages/sklearn/model_selection/_search.py:875\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    869\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    870\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    871\u001b[0m     )\n\u001b[1;32m    873\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 875\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    878\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    879\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/soil_env2/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1375\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1375\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/soil_env2/lib/python3.10/site-packages/sklearn/model_selection/_search.py:822\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    816\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    817\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    818\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    819\u001b[0m         )\n\u001b[1;32m    820\u001b[0m     )\n\u001b[0;32m--> 822\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    840\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    841\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    842\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    843\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    844\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/soil_env2/lib/python3.10/site-packages/joblib/parallel.py:1056\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1053\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1056\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1058\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/soil_env2/lib/python3.10/site-packages/joblib/parallel.py:935\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    934\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 935\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(\u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    936\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    937\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/soil_env2/lib/python3.10/site-packages/joblib/_parallel_backends.py:542\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 542\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    544\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/soil_env2/lib/python3.10/concurrent/futures/_base.py:441\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 441\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/soil_env2/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for file_response in os.listdir(path_response):  \n",
    "    if (file_response != '.DS_Store') & (file_response != 'Icon\\r'):  \n",
    "        if (file_response == 'Yield_Meter') or  (file_response == 'Yield_Plant'):\n",
    "            print(file_response)  \n",
    "            path_r= path_response+file_response  \n",
    "            os.chdir(path_r)  \n",
    "            for re in os.listdir(path_r):  \n",
    "                if re[0:8] == 'response':  \n",
    "                    response = pd.read_csv(path_response+file_response+'/'+re)  \n",
    "                    response.rename(columns={'Column1':'Link_ID'}, inplace=True)  \n",
    "                    response.rename(columns={response.columns[1]:'y_c'}, inplace=True)  \n",
    "\n",
    "                    writer= pd.ExcelWriter(path_r+'/'+'feature_selection'+'.xlsx', engine='xlsxwriter')   \n",
    "                    for file_folder in os.listdir(path_x):  \n",
    "                        if (file_folder[-4:] != '.csv') & (file_folder != '.DS_Store')& (file_folder != 'Icon\\r'):          \n",
    "                            path = path_x+file_folder  \n",
    "                            os.chdir(path)  \n",
    "                            file_list = []  \n",
    "\n",
    "                            for file in os.listdir(path):  \n",
    "                                if (file[0] != 't') & (file[-4:] == '.csv') & (file != '.DS_Store'):  \n",
    "                                    print(file)  \n",
    "                                    file_list.append(file)  \n",
    "                                    data_temp = pd.read_csv(file)  \n",
    "                                    data_temp.rename(columns={'Unnamed: 0':'Link_ID'}, inplace=True)  \n",
    "                                    data=pd.merge(response,data_temp,on='Link_ID')  \n",
    "                                    data.drop(columns = 'Link_ID',inplace=True)  \n",
    "                                    data_train,data_val = train_test_split(data,train_size=0.8, random_state=42) \n",
    "                                    output = process_data(data,data_train,data_val,cv,q) \n",
    "                            output.to_excel(writer, sheet_name=file_folder, index=True)    \n",
    "                    writer.save()                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "360e9b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_tuber_scab\n",
      " 1 _ 1 .csv\n",
      "Fitting 30 folds for each of 36 candidates, totalling 1080 fits\n",
      "Fitting 30 folds for each of 36 candidates, totalling 1080 fits\n",
      " 1 _ 1 .csv\n",
      "Fitting 30 folds for each of 36 candidates, totalling 1080 fits\n",
      "Fitting 30 folds for each of 36 candidates, totalling 1080 fits\n",
      " 1 _ 1 .csv\n",
      "Fitting 30 folds for each of 36 candidates, totalling 1080 fits\n",
      "Fitting 30 folds for each of 36 candidates, totalling 1080 fits\n",
      " 1 _ 1 .csv\n",
      "Fitting 30 folds for each of 36 candidates, totalling 1080 fits\n",
      "Fitting 30 folds for each of 36 candidates, totalling 1080 fits\n",
      " 1 _ 1 .csv\n",
      "Fitting 30 folds for each of 36 candidates, totalling 1080 fits\n",
      "Fitting 30 folds for each of 36 candidates, totalling 1080 fits\n",
      "no_tuber_scabpit\n",
      " 1 _ 1 .csv\n",
      "Fitting 30 folds for each of 36 candidates, totalling 1080 fits\n",
      "Fitting 30 folds for each of 36 candidates, totalling 1080 fits\n",
      " 1 _ 1 .csv\n",
      "Fitting 30 folds for each of 36 candidates, totalling 1080 fits\n",
      "Fitting 30 folds for each of 36 candidates, totalling 1080 fits\n",
      " 1 _ 1 .csv\n",
      "Fitting 30 folds for each of 36 candidates, totalling 1080 fits\n",
      "Fitting 30 folds for each of 36 candidates, totalling 1080 fits\n",
      " 1 _ 1 .csv\n",
      "Fitting 30 folds for each of 36 candidates, totalling 1080 fits\n",
      "Fitting 30 folds for each of 36 candidates, totalling 1080 fits\n",
      " 1 _ 1 .csv\n",
      "Fitting 30 folds for each of 36 candidates, totalling 1080 fits\n",
      "Fitting 30 folds for each of 36 candidates, totalling 1080 fits\n",
      "no_tuber_scabsuper\n",
      " 1 _ 1 .csv\n",
      "Fitting 30 folds for each of 36 candidates, totalling 1080 fits\n",
      "Fitting 30 folds for each of 36 candidates, totalling 1080 fits\n",
      " 1 _ 1 .csv\n",
      "Fitting 30 folds for each of 36 candidates, totalling 1080 fits\n",
      "Fitting 30 folds for each of 36 candidates, totalling 1080 fits\n",
      " 1 _ 1 .csv\n",
      "Fitting 30 folds for each of 36 candidates, totalling 1080 fits\n",
      "Fitting 30 folds for each of 36 candidates, totalling 1080 fits\n",
      " 1 _ 1 .csv\n",
      "Fitting 30 folds for each of 36 candidates, totalling 1080 fits\n",
      "Fitting 30 folds for each of 36 candidates, totalling 1080 fits\n",
      " 1 _ 1 .csv\n",
      "Fitting 30 folds for each of 36 candidates, totalling 1080 fits\n",
      "Fitting 30 folds for each of 36 candidates, totalling 1080 fits\n"
     ]
    }
   ],
   "source": [
    "  for file_response in os.listdir(path_response):  \n",
    "    if (file_response != '.DS_Store') & (file_response != 'Icon\\r'):  \n",
    "        if (file_response != 'Yield_Meter') &  (file_response != 'Yield_Plant'):\n",
    "            print(file_response)  \n",
    "            path_r= path_response+file_response  \n",
    "            os.chdir(path_r)  \n",
    "            for re in os.listdir(path_r):  \n",
    "                if re[0:8] == 'response':  \n",
    "                    response = pd.read_csv(path_response+file_response+'/'+re)  \n",
    "                    response.rename(columns={'Column1':'Link_ID',response.columns[1]:'y_b'}, inplace=True)    \n",
    "                    writer= pd.ExcelWriter(path_r+'/'+'feature_selection'+'.xlsx', engine='xlsxwriter')   \n",
    "                    for file_folder in os.listdir(path_x):  \n",
    "                        if (file_folder[-4:] != '.csv') & (file_folder != '.DS_Store')& (file_folder != 'Icon\\r'):          \n",
    "                            path = path_x+file_folder  \n",
    "                            os.chdir(path)  \n",
    "                            file_list = []  \n",
    "                            tRF=pd.DataFrame()  \n",
    "                            tcluster=pd.DataFrame()  \n",
    "                            k=0  \n",
    "                            for file in os.listdir(path):  \n",
    "                                if (file[0] != 't') & (file[-4:] == '.csv') & (file != '.DS_Store'):  \n",
    "                                    print(file)  \n",
    "                                    file_list.append(file)  \n",
    "                                    data_temp = pd.read_csv(file)  \n",
    "                                    data_temp.rename(columns={'Unnamed: 0':'Link_ID'}, inplace=True)  \n",
    "                                    data=pd.merge(response,data_temp,on='Link_ID')  \n",
    "                                    data.drop(columns = 'Link_ID',inplace=True)  \n",
    "                                    data_train,data_val = train_test_split(data,train_size=0.8, random_state=42) \n",
    "                                    output = process_data_binary(data,data_train,data_val,cv,q) \n",
    "                            output.to_excel(writer, sheet_name=file_folder, index=True)    \n",
    "                    writer.save()                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f1065f",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = datetime.now()\n",
    "print('Duration: {}'.format(end_time - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
