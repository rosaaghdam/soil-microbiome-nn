{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af999b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.feature_selection import SelectKBest,mutual_info_regression,RFE, f_classif,mutual_info_classif\n",
    "from sklearn.linear_model import  LassoCV,LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor,GradientBoostingClassifier,RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV,train_test_split,RepeatedKFold\n",
    "import os\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84d588f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv=RepeatedKFold(n_splits=10,n_repeats=3, random_state=100)\n",
    "q=0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "815f989d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e512479b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stepwise_selection(data, target,SL_in=0.05,SL_out = 0.05):\n",
    "    initial_features = data.columns.tolist()\n",
    "    best_features = []\n",
    "    while (len(initial_features)>0):\n",
    "        remaining_features = list(set(initial_features)-set(best_features))\n",
    "        new_pval = pd.Series(index=remaining_features)\n",
    "        for new_column in remaining_features:\n",
    "            model = sm.OLS(target, sm.add_constant(data[best_features+[new_column]])).fit()\n",
    "            new_pval[new_column] = model.pvalues[new_column]\n",
    "        min_p_value = new_pval.min()\n",
    "        if(min_p_value<SL_in):\n",
    "            best_features.append(new_pval.idxmin())\n",
    "            while(len(best_features)>0):\n",
    "                best_features_with_constant = sm.add_constant(data[best_features])\n",
    "                p_values = sm.OLS(target, best_features_with_constant).fit().pvalues[1:]\n",
    "                max_p_value = p_values.max()\n",
    "                if(max_p_value >= SL_out):\n",
    "                    excluded_feature = p_values.idxmax()\n",
    "                    best_features.remove(excluded_feature)\n",
    "                else:\n",
    "                    break \n",
    "        else:\n",
    "            break\n",
    "    return best_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2223a6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data,data_train,data_val,cv,q):  \n",
    "    x_column_list = data_train.drop(columns=['y_c']).columns \n",
    "    feature_list1 = data[x_column_list].max().sort_values(ascending=False)[data[x_column_list].max()>np.quantile(data[x_column_list].max(),q)].index\n",
    "    #apply SelectKBest class to extract top 20 best features\n",
    "    bestfeatures = SelectKBest(score_func=mutual_info_regression, k=round(data.shape[1]*(1-q)))\n",
    "    fit = bestfeatures.fit(data[x_column_list],data['y_c'])\n",
    "    dfscores = pd.DataFrame(fit.scores_)\n",
    "    dfcolumns = pd.DataFrame(x_column_list)\n",
    "    #concat two dataframes for better visualization \n",
    "    featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "    featureScores.columns = ['Variable','Score']  #naming the dataframe columns\n",
    "    feature_list2 = featureScores.nlargest(round(data.shape[1]*(1-q)),'Score')\n",
    "    \n",
    "    \n",
    "    feature_list3=stepwise_selection(data[x_column_list],data['y_c'])\n",
    "    reg = LassoCV(cv=cv).fit(data_train[x_column_list], data_train['y_c'])\n",
    "    lassoCoefs0 = pd.DataFrame(\n",
    "    data=reg.coef_[np.where(reg.coef_ != 0)[0]], \n",
    "    index=data[x_column_list].columns[np.where(reg.coef_ != 0)[0]],columns=['LASSO Coefs1'])\n",
    "    feature_list4=lassoCoefs0\n",
    "    \n",
    "    parameters = {'n_estimators':(100, 500),\n",
    "                  'min_samples_split':(3,4,5),\n",
    "                  'min_samples_leaf':(3,4,5)}\n",
    "\n",
    "    gb_model = GradientBoostingRegressor(random_state=7, warm_start=False)\n",
    "    grid_obj = GridSearchCV(gb_model, param_grid=parameters, verbose=1, n_jobs=4, cv=cv)\n",
    "    grid_obj = grid_obj.fit(data_train[x_column_list],data_train['y_c'])\n",
    "    gb_model_best = grid_obj.best_estimator_\n",
    "    y_hat = gb_model_best.predict(data_val[x_column_list])\n",
    "    FeatImportance = gb_model_best.feature_importances_\n",
    "    GBCoefs = pd.DataFrame(index=data_train[x_column_list].columns, data=FeatImportance,columns=['Coefs'])\n",
    "    imp_coef = GBCoefs.sort_values(by='Coefs')\n",
    "    feature_list6=imp_coef.loc[imp_coef['Coefs']>np.quantile(imp_coef,q)]\n",
    "    feature_list6 = [item for item in feature_list6.index]\n",
    "    parameters = {'n_estimators':(10,20,100, 500),\n",
    "              'min_samples_split':(2,3,4),\n",
    "              'min_samples_leaf':(1,2,3)}\n",
    "    rf_model = RandomForestRegressor(warm_start=False)\n",
    "    grid_obj = GridSearchCV(rf_model, param_grid=parameters, verbose=1, n_jobs=4, cv=cv)\n",
    "    grid_obj = grid_obj.fit(data_train[x_column_list],data_train['y_c'])\n",
    "    rf_model_best = grid_obj.best_estimator_\n",
    "    y_hat_rf = rf_model_best.predict(data_val[x_column_list])  \n",
    "    rf_model_best.score(data_val[x_column_list],data_val['y_c'])\n",
    "    FeatImportance = rf_model_best.feature_importances_\n",
    "    RFCoefs = pd.DataFrame(index=data_train[x_column_list].columns, data=FeatImportance,columns=['Coefs'])\n",
    "    imp_coef_RF = RFCoefs.sort_values(by='Coefs')\n",
    "    feature_list7=imp_coef_RF.loc[imp_coef_RF['Coefs']>np.quantile(imp_coef_RF,q)]\n",
    "    feature_list7 = [item for item in feature_list7.index]\n",
    "    \n",
    "    mi = mutual_info_regression(data_train[x_column_list],data_train['y_c'], discrete_features=False, n_neighbors=3, copy=True, random_state=None)\n",
    "    MI=pd.DataFrame(mi)\n",
    "    MI['OTU']=data[x_column_list].columns\n",
    "    MI=MI.set_index('OTU')\n",
    "    MI=MI.sort_values(0,ascending=False)\n",
    "    feature_list8=MI[MI[0]>np.quantile(mi,q)].index\n",
    "    \n",
    "    methodList = ['Maximum', \n",
    "              'KBest', \n",
    "              'Stepwise Regression', \n",
    "              'Lasso CV', \n",
    "              'GBM', \n",
    "              'Random Forest', \n",
    "              'Mutual Info']\n",
    "    featureList = [[item for item in feature_list1], \n",
    "               [item for item in feature_list2['Variable'].values], \n",
    "               feature_list3, \n",
    "               [item for item in feature_list4.index], \n",
    "               feature_list6, \n",
    "               feature_list7,\n",
    "               [item for item in feature_list8]]\n",
    "    featureUniqueList = np.unique([item for sublist in featureList for item in sublist])\n",
    "    featureDictionary = dict.fromkeys(featureUniqueList)\n",
    "    for key in featureDictionary.keys():\n",
    "        featureDictionary[key] = []\n",
    "    \n",
    "\n",
    "    for feature in featureUniqueList:\n",
    "    \n",
    "        for i, method in enumerate(methodList):\n",
    "        \n",
    "            if feature in featureList[i]:\n",
    "                featureDictionary[feature].append(method)   \n",
    "\n",
    "    featureDf = pd.DataFrame(index=featureUniqueList, columns=methodList)\n",
    "    featureDf['Count'] = 0\n",
    "\n",
    "    for feature in featureDictionary.keys(): \n",
    "        for method in methodList:\n",
    "        \n",
    "            if method in featureDictionary[feature]:\n",
    "            \n",
    "                featureDf['Count'][feature] += 1\n",
    "            \n",
    "                featureDf[method][feature] = 'X'\n",
    "            else:\n",
    "                featureDf[method][feature] = '-' \n",
    "    featureDf.sort_values(by='Count', ascending=False, inplace=True)\n",
    "    return featureDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a3081bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_binary(data,data_train,data_val,cv,q):  \n",
    "    x_column_list = data.drop(columns=['y_b']).columns \n",
    "    feature_list1 = data[x_column_list].max().sort_values(ascending=False)[data[x_column_list].max()>np.quantile(data[x_column_list].max(),q)].index\n",
    "    #apply SelectKBest class to extract top  best features\n",
    "    bestfeatures = SelectKBest(score_func=f_classif, k=round(data.shape[1]*(1-q)))\n",
    "    fit = bestfeatures.fit(data[x_column_list],data['y_b'])\n",
    "    dfscores = pd.DataFrame(fit.scores_)\n",
    "    dfcolumns = pd.DataFrame(x_column_list)\n",
    "    #concat two dataframes for better visualization \n",
    "    featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "    featureScores.columns = ['Variable','Score']  #naming the dataframe columns\n",
    "    feature_list2 = featureScores.nlargest(round(data.shape[1]*(1-q)),'Score')\n",
    "    \n",
    "    #Recursive Feature Elimination LogisticRegression\n",
    "    model = LogisticRegression(solver='lbfgs')\n",
    "    rfe = RFE(model, n_features_to_select = round(data.shape[1]*(1-q)))\n",
    "    fit = rfe.fit(data[x_column_list], data['y_b'])\n",
    "    feature_list3 = data[x_column_list].columns[fit.ranking_==1]\n",
    "\n",
    "    #Recursive Feature DecisionTree\n",
    "    model = DecisionTreeClassifier()\n",
    "    rfe = RFE(model, n_features_to_select = round(data.shape[1]*(1-q)))\n",
    "    fit = rfe.fit(data[x_column_list], data['y_b'])\n",
    "    feature_list4 = data[x_column_list].columns[fit.ranking_==1]\n",
    "    \n",
    "    #GradientBoostingClassifier\n",
    "    parameters = {'n_estimators':(100, 500),\n",
    "              'min_samples_split':(3,4),\n",
    "              'min_samples_leaf':(3,4,5)}\n",
    "\n",
    "    gb_model = GradientBoostingClassifier(random_state=7, warm_start=False)\n",
    "    grid_obj = GridSearchCV(gb_model, param_grid=parameters, verbose=1, n_jobs=4, cv=cv)\n",
    "    grid_obj = grid_obj.fit(data_train[x_column_list],data_train['y_b'])\n",
    "    gb_model_best = grid_obj.best_estimator_\n",
    "    FeatImportance = gb_model_best.feature_importances_\n",
    "    GBCoefs = pd.DataFrame(index=data_train[x_column_list].columns, data=FeatImportance,columns=['Coefs'])\n",
    "    imp_coef = GBCoefs.sort_values(by='Coefs')\n",
    "    feature_list6=imp_coef.loc[imp_coef['Coefs']>np.quantile(imp_coef,q)]\n",
    "    feature_list6 = [item for item in feature_list6.index]\n",
    "\n",
    "    # RandomForestClassifier\n",
    "    parameters = {'n_estimators':(10,20,100, 500),\n",
    "          'min_samples_split':(2,3,4),\n",
    "          'min_samples_leaf':(1,2,3)}\n",
    "    rf_model = RandomForestClassifier(warm_start=False)\n",
    "    grid_obj = GridSearchCV(rf_model, param_grid=parameters, verbose=1, n_jobs=-1, cv=cv)\n",
    "    grid_obj = grid_obj.fit(data_train[x_column_list],data_train['y_b'])\n",
    "    rf_model_best = grid_obj.best_estimator_\n",
    "    y_hat_rf = rf_model_best.predict(data_val[x_column_list])  \n",
    "    FeatImportance = rf_model_best.feature_importances_\n",
    "    RFCoefs = pd.DataFrame(index=data_train[x_column_list].columns, data=FeatImportance,columns=['Coefs'])\n",
    "    imp_coef_RF = RFCoefs.sort_values(by='Coefs')\n",
    "    feature_list7=imp_coef_RF.loc[imp_coef_RF['Coefs']>np.quantile(imp_coef_RF,q)]\n",
    "    feature_list7 = [item for item in feature_list7.index]\n",
    "    #mutual_info_classif\n",
    "    mi = mutual_info_classif(data_train[x_column_list],data_train['y_b'], discrete_features=False, n_neighbors=3, copy=True, random_state=None)\n",
    "    MI=pd.DataFrame(mi)\n",
    "    MI['OTU']=data[x_column_list].columns\n",
    "    MI=MI.set_index('OTU')\n",
    "    MI=MI.sort_values(0,ascending=False)\n",
    "    feature_list8=MI[MI[0]>np.quantile(mi,q)].index\n",
    "    \n",
    "    methodList = ['Maximum', \n",
    "              'KBest', \n",
    "              'RFE_logistic', \n",
    "              'RFE_RF', \n",
    "              'GBM', \n",
    "              'Random Forest', \n",
    "              'Mutual Info']\n",
    "    featureList = [[item for item in feature_list1], \n",
    "               [item for item in feature_list2['Variable'].values], \n",
    "               feature_list3, \n",
    "               [item for item in feature_list4], \n",
    "               feature_list6, \n",
    "               feature_list7,\n",
    "               [item for item in feature_list8]]\n",
    "    featureUniqueList = np.unique([item for sublist in featureList for item in sublist])\n",
    "    featureDictionary = dict.fromkeys(featureUniqueList)\n",
    "    for key in featureDictionary.keys():\n",
    "        featureDictionary[key] = []\n",
    "    \n",
    "\n",
    "    for feature in featureUniqueList:\n",
    "    \n",
    "        for i, method in enumerate(methodList):\n",
    "        \n",
    "            if feature in featureList[i]:\n",
    "                featureDictionary[feature].append(method)   \n",
    "\n",
    "    featureDf = pd.DataFrame(index=featureUniqueList, columns=methodList)\n",
    "    featureDf['Count'] = 0\n",
    "\n",
    "    for feature in featureDictionary.keys(): \n",
    "        for method in methodList:\n",
    "        \n",
    "            if method in featureDictionary[feature]:\n",
    "            \n",
    "                featureDf['Count'][feature] += 1\n",
    "            \n",
    "                featureDf[method][feature] = 'X'\n",
    "            else:\n",
    "                featureDf[method][feature] = '-' \n",
    "    featureDf.sort_values(by='Count', ascending=False, inplace=True)\n",
    "    return featureDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "584b03d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38d1d6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_local='/Users/rosa/Desktop/ALLWork/Madison/Project/Soil-nn/Code/python code local/Github_file/'\n",
    "path_x = path_local+'OTUData-1-1/'  \n",
    "path_response=path_local+'disease_response/'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f0fd60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yield_Plant\n",
      " 1 _ 1 .csv\n",
      "Fitting 30 folds for each of 18 candidates, totalling 540 fits\n"
     ]
    }
   ],
   "source": [
    "for file_response in os.listdir(path_response):  \n",
    "    if (file_response != '.DS_Store') & (file_response != 'Icon\\r'):  \n",
    "        if (file_response == 'Yield_Meter') or  (file_response == 'Yield_Plant'):\n",
    "            print(file_response)  \n",
    "            path_r= path_response+file_response  \n",
    "            os.chdir(path_r)  \n",
    "            for re in os.listdir(path_r):  \n",
    "                if re[0:8] == 'response':  \n",
    "                    response = pd.read_csv(path_response+file_response+'/'+re)  \n",
    "                    response.rename(columns={'Column1':'Link_ID'}, inplace=True)  \n",
    "                    response.rename(columns={response.columns[1]:'y_c'}, inplace=True)  \n",
    "\n",
    "                    writer= pd.ExcelWriter(path_r+'/'+'feature_selection'+'.xlsx', engine='xlsxwriter')   \n",
    "                    for file_folder in os.listdir(path_x):  \n",
    "                        if (file_folder[-4:] != '.csv') & (file_folder != '.DS_Store')& (file_folder != 'Icon\\r'):          \n",
    "                            path = path_x+file_folder  \n",
    "                            os.chdir(path)  \n",
    "                            file_list = []  \n",
    "\n",
    "                            for file in os.listdir(path):  \n",
    "                                if (file[0] != 't') & (file[-4:] == '.csv') & (file != '.DS_Store'):  \n",
    "                                    print(file)  \n",
    "                                    file_list.append(file)  \n",
    "                                    data_temp = pd.read_csv(file)  \n",
    "                                    data_temp.rename(columns={'Unnamed: 0':'Link_ID'}, inplace=True)  \n",
    "                                    data=pd.merge(response,data_temp,on='Link_ID')  \n",
    "                                    data.drop(columns = 'Link_ID',inplace=True)  \n",
    "                                    data_train,data_val = train_test_split(data,train_size=0.8, random_state=42) \n",
    "                                    output = process_data(data,data_train,data_val,cv,q) \n",
    "                            output.to_excel(writer, sheet_name=file_folder, index=True)    \n",
    "                    writer.save()                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "360e9b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_tuber_scab\n",
      " 1 _ 1 .csv\n",
      "Fitting 30 folds for each of 36 candidates, totalling 1080 fits\n",
      "Fitting 30 folds for each of 36 candidates, totalling 1080 fits\n",
      " 1 _ 1 .csv\n",
      "Fitting 30 folds for each of 36 candidates, totalling 1080 fits\n",
      "Fitting 30 folds for each of 36 candidates, totalling 1080 fits\n",
      " 1 _ 1 .csv\n",
      "Fitting 30 folds for each of 36 candidates, totalling 1080 fits\n",
      "Fitting 30 folds for each of 36 candidates, totalling 1080 fits\n",
      " 1 _ 1 .csv\n",
      "Fitting 30 folds for each of 36 candidates, totalling 1080 fits\n",
      "Fitting 30 folds for each of 36 candidates, totalling 1080 fits\n",
      " 1 _ 1 .csv\n",
      "Fitting 30 folds for each of 36 candidates, totalling 1080 fits\n",
      "Fitting 30 folds for each of 36 candidates, totalling 1080 fits\n",
      "no_tuber_scabpit\n",
      " 1 _ 1 .csv\n",
      "Fitting 30 folds for each of 36 candidates, totalling 1080 fits\n",
      "Fitting 30 folds for each of 36 candidates, totalling 1080 fits\n",
      " 1 _ 1 .csv\n",
      "Fitting 30 folds for each of 36 candidates, totalling 1080 fits\n",
      "Fitting 30 folds for each of 36 candidates, totalling 1080 fits\n",
      " 1 _ 1 .csv\n",
      "Fitting 30 folds for each of 36 candidates, totalling 1080 fits\n",
      "Fitting 30 folds for each of 36 candidates, totalling 1080 fits\n",
      " 1 _ 1 .csv\n",
      "Fitting 30 folds for each of 36 candidates, totalling 1080 fits\n",
      "Fitting 30 folds for each of 36 candidates, totalling 1080 fits\n",
      " 1 _ 1 .csv\n",
      "Fitting 30 folds for each of 36 candidates, totalling 1080 fits\n",
      "Fitting 30 folds for each of 36 candidates, totalling 1080 fits\n",
      "no_tuber_scabsuper\n",
      " 1 _ 1 .csv\n",
      "Fitting 30 folds for each of 36 candidates, totalling 1080 fits\n",
      "Fitting 30 folds for each of 36 candidates, totalling 1080 fits\n",
      " 1 _ 1 .csv\n",
      "Fitting 30 folds for each of 36 candidates, totalling 1080 fits\n",
      "Fitting 30 folds for each of 36 candidates, totalling 1080 fits\n",
      " 1 _ 1 .csv\n",
      "Fitting 30 folds for each of 36 candidates, totalling 1080 fits\n",
      "Fitting 30 folds for each of 36 candidates, totalling 1080 fits\n",
      " 1 _ 1 .csv\n",
      "Fitting 30 folds for each of 36 candidates, totalling 1080 fits\n",
      "Fitting 30 folds for each of 36 candidates, totalling 1080 fits\n",
      " 1 _ 1 .csv\n",
      "Fitting 30 folds for each of 36 candidates, totalling 1080 fits\n",
      "Fitting 30 folds for each of 36 candidates, totalling 1080 fits\n"
     ]
    }
   ],
   "source": [
    "  for file_response in os.listdir(path_response):  \n",
    "    if (file_response != '.DS_Store') & (file_response != 'Icon\\r'):  \n",
    "        if (file_response != 'Yield_Meter') &  (file_response != 'Yield_Plant'):\n",
    "            print(file_response)  \n",
    "            path_r= path_response+file_response  \n",
    "            os.chdir(path_r)  \n",
    "            for re in os.listdir(path_r):  \n",
    "                if re[0:8] == 'response':  \n",
    "                    response = pd.read_csv(path_response+file_response+'/'+re)  \n",
    "                    response.rename(columns={'Column1':'Link_ID',response.columns[1]:'y_b'}, inplace=True)    \n",
    "                    writer= pd.ExcelWriter(path_r+'/'+'feature_selection'+'.xlsx', engine='xlsxwriter')   \n",
    "                    for file_folder in os.listdir(path_x):  \n",
    "                        if (file_folder[-4:] != '.csv') & (file_folder != '.DS_Store')& (file_folder != 'Icon\\r'):          \n",
    "                            path = path_x+file_folder  \n",
    "                            os.chdir(path)  \n",
    "                            file_list = []  \n",
    "                            tRF=pd.DataFrame()  \n",
    "                            tcluster=pd.DataFrame()  \n",
    "                            k=0  \n",
    "                            for file in os.listdir(path):  \n",
    "                                if (file[0] != 't') & (file[-4:] == '.csv') & (file != '.DS_Store'):  \n",
    "                                    print(file)  \n",
    "                                    file_list.append(file)  \n",
    "                                    data_temp = pd.read_csv(file)  \n",
    "                                    data_temp.rename(columns={'Unnamed: 0':'Link_ID'}, inplace=True)  \n",
    "                                    data=pd.merge(response,data_temp,on='Link_ID')  \n",
    "                                    data.drop(columns = 'Link_ID',inplace=True)  \n",
    "                                    data_train,data_val = train_test_split(data,train_size=0.8, random_state=42) \n",
    "                                    output = process_data_binary(data,data_train,data_val,cv,q) \n",
    "                            output.to_excel(writer, sheet_name=file_folder, index=True)    \n",
    "                    writer.save()                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f1065f",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = datetime.now()\n",
    "print('Duration: {}'.format(end_time - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
