{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a55884a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux, Statistics\n",
    "using Flux: onehotbatch, onecold, crossentropy, throttle, params \n",
    "using Base.Iterators: repeated, partition\n",
    "using Printf, BSON\n",
    "using CSV\n",
    "using DataFrames\n",
    "using Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4300640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of low-yield samples: 222; Number of high-yield samples: 222\n"
     ]
    }
   ],
   "source": [
    "original_dat = CSV.read(\"../processed-data/otu-yield-per-plant.csv\", header=true, DataFrame)\n",
    "generated_dat = CSV.read(\"../processed-data/generated-data.csv\", header=false, DataFrame)\n",
    "\n",
    "original = Matrix(original_dat)\n",
    "generated = Matrix(generated_dat)\n",
    "full_data = vcat(original, generated)\n",
    "\n",
    "otu = full_data[:,2:60]\n",
    "label = full_data[:,62]\n",
    "\n",
    "percentage = [(i, count(==(i), label)) for i in unique(label)]\n",
    "println(\"Number of low-yield samples: \", percentage[1][2], \"; Number of high-yield samples: \", percentage[2][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cec15f60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "make_fold (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function make_fold(data, idx)\n",
    "    # The 2D array for data in each folder.The dimension is 22*2394\n",
    "    data_batch = Array{Float32, 2}(undef, length(idx), length(data[1,:]))\n",
    "    # Add all data for this folder into the batch\n",
    "    for i in 1:length(idx)\n",
    "        data_batch[i,:] = data[idx[i],:]\n",
    "    end\n",
    "    return (data_batch', data_batch')\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff74b2d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "k_fold_partition (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function k_fold_partition(otu_batch)\n",
    "    # partition the whole dataset into 10 folds\n",
    "    fold_idx = partition(1:length(otu_batch[:,1]), length(otu_batch[:,1])÷10+1)\n",
    "    # call make_fold and store the 10 folds\n",
    "    whole_set = [make_fold(otu_batch, i) for i in fold_idx]\n",
    "\n",
    "    return whole_set\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c7983673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  Chain(\n",
       "    Dense(59 => 32, σ),                 \u001b[90m# 1_920 parameters\u001b[39m\n",
       "    Dense(32 => 5),                     \u001b[90m# 165 parameters\u001b[39m\n",
       "  ),\n",
       "  Chain(\n",
       "    Dense(5 => 32, σ),                  \u001b[90m# 192 parameters\u001b[39m\n",
       "    Dense(32 => 59, σ),                 \u001b[90m# 1_947 parameters\u001b[39m\n",
       "  ),\n",
       ") \u001b[90m                  # Total: 8 arrays, \u001b[39m4_224 parameters, 17.125 KiB."
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Chain(\n",
    "    Chain(\n",
    "    Dense(59, 32, σ),\n",
    "    #Dense(40, 25, σ),\n",
    "    Dense(32, 5)),\n",
    "    Chain(\n",
    "    Dense(5, 32, σ),\n",
    "    #Dense(25, 40, σ),\n",
    "    Dense(32, 59, σ))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e0891f9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train (generic function with 1 method)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function train(whole_set) \n",
    "    uni_best = 1\n",
    "    # loop through all 10 folders\n",
    "    for k in 1:10 \n",
    "        println(\"Start training on the \", k, \"th fold...\")\n",
    "        # set the training set and the testing set\n",
    "        train_set = whole_set[Not(k)]\n",
    "        test_set = whole_set[k]\n",
    "        # reset all the parameters\n",
    "        Flux.loadparams!(model, map(p -> p .= randn.(), Flux.params(model)))\n",
    "        # here the loss function is MSE, I also tried cross-entropy. \n",
    "        # I'll write down the result for both in the conclusion section\n",
    "        loss(x, y) = Flux.Losses.mse(model(x),y)\n",
    "        # record the number of continuous epoches that the loss increases\n",
    "        loss_inc = 0\n",
    "        # the loss of the current epoch for the validation set\n",
    "        val_loss = 1000\n",
    "        # the lowest loss so far in this fold\n",
    "        best_loss = 1000\n",
    "\n",
    "        # the optimizer is Adam with learning rate of 0.001\n",
    "        opt = ADAM(0.001)\n",
    "        # the maximum epoch is 200\n",
    "        for epoch_idx in 1:10000\n",
    "            # train the network\n",
    "            Flux.train!(loss, params(model), train_set, opt)\n",
    "            # calculate the validation loss for this epoch\n",
    "            val_loss = loss(test_set...)\n",
    "            # if the loss increases, increment the counter\n",
    "            if val_loss >= best_loss \n",
    "                loss_inc += 1\n",
    "            else\n",
    "                # if not, then set the current loss as lowest\n",
    "                best_loss = val_loss\n",
    "                loss_inc = 0\n",
    "                if best_loss < uni_best\n",
    "                    uni_best = best_loss\n",
    "                    BSON.@save \"AENN.bson\" model epoch_idx uni_best\n",
    "                end\n",
    "            end\n",
    "            # loss has increased for 5 continuous epoch, exit to prevent overfitting\n",
    "            if loss_inc >= 5 && opt.eta > 1e-8\n",
    "                opt.eta /= 10.0\n",
    "                loss_inc = 0\n",
    "            end\n",
    "            \n",
    "            if loss_inc >= 10\n",
    "                println(\"Force exit to prevent overfit at epoch: \", epoch_idx)\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "        println(\"Finished training the \", k, \"th folder...\" )\n",
    "        println(\"The final validation loss is: \", best_loss)\n",
    "        println(\"------------------------------------\")\n",
    "    end\n",
    "    return uni_best\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d4cb8a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The untrained loss for each folder is: 0.4032061, 0.40256885, 0.4030643, 0.4028901, 0.401919, 0.40236816, 0.40230292, 0.40226257, 0.40237787, 0.40295884, Start training...\n",
      "Start training on the 1th fold...\n",
      "Force exit to prevent overfit at epoch: 1623\n",
      "Finished training the 1th folder...\n",
      "The final validation loss is: 2.0491874e-5\n",
      "------------------------------------\n",
      "Start training on the 2th fold...\n",
      "Force exit to prevent overfit at epoch: 1424\n",
      "Finished training the 2th folder...\n",
      "The final validation loss is: 4.5302775e-5\n",
      "------------------------------------\n",
      "Start training on the 3th fold...\n",
      "Force exit to prevent overfit at epoch: 1997\n",
      "Finished training the 3th folder...\n",
      "The final validation loss is: 3.443326e-5\n",
      "------------------------------------\n",
      "Start training on the 4th fold...\n",
      "Finished training the 4th folder...\n",
      "The final validation loss is: 2.2111126e-5\n",
      "------------------------------------\n",
      "Start training on the 5th fold...\n",
      "Force exit to prevent overfit at epoch: 1178\n",
      "Finished training the 5th folder...\n",
      "The final validation loss is: 2.9559122e-5\n",
      "------------------------------------\n",
      "Start training on the 6th fold...\n",
      "Force exit to prevent overfit at epoch: 1906\n",
      "Finished training the 6th folder...\n",
      "The final validation loss is: 1.896143e-5\n",
      "------------------------------------\n",
      "Start training on the 7th fold...\n",
      "Force exit to prevent overfit at epoch: 1154\n",
      "Finished training the 7th folder...\n",
      "The final validation loss is: 3.8300543e-5\n",
      "------------------------------------\n",
      "Start training on the 8th fold...\n",
      "Force exit to prevent overfit at epoch: 1410\n",
      "Finished training the 8th folder...\n",
      "The final validation loss is: 4.2970725e-5\n",
      "------------------------------------\n",
      "Start training on the 9th fold...\n",
      "Force exit to prevent overfit at epoch: 1385\n",
      "Finished training the 9th folder...\n",
      "The final validation loss is: 2.98916e-5\n",
      "------------------------------------\n",
      "Start training on the 10th fold...\n",
      "Force exit to prevent overfit at epoch: 1227\n",
      "Finished training the 10th folder...\n",
      "The final validation loss is: 2.8772121e-5\n",
      "------------------------------------\n",
      "The lowest loss among all models is: 1.896143e-5\n"
     ]
    }
   ],
   "source": [
    "whole_set = k_fold_partition(otu)\n",
    "# reset the parameter of the model to get the untrained loss, just as a reference\n",
    "Flux.loadparams!(model, map(p -> p .= randn.(), Flux.params(model)))\n",
    "loss(x, y) = Flux.Losses.mse(model(x),y)\n",
    "print(\"The untrained loss for each folder is: \")\n",
    "for i in 1:10\n",
    "    print(loss(whole_set[i]...), \", \")\n",
    "end\n",
    "\n",
    "# Start the training\n",
    "println(\"Start training...\")\n",
    "best_loss = train(whole_set)\n",
    "println(\"The lowest loss among all models is: \", best_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0aad0ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "BSON.@load \"AENN.bson\" model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "69054774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"../processed-data/plot.csv\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = otu[1,:]\n",
    "b = model(otu[1,:])\n",
    "d = otu[5,:]\n",
    "e = model(otu[5,:])\n",
    "c = hcat(a,b,d,e)\n",
    "CSV.write(\"../processed-data/plot.csv\",  Tables.table(c), writeheader=false)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.3",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
