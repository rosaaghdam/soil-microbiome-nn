{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a55884a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux, Statistics\n",
    "using Flux: onehotbatch, onecold, crossentropy, throttle, params \n",
    "using Base.Iterators: repeated, partition\n",
    "using Printf, BSON\n",
    "using CSV\n",
    "using DataFrames\n",
    "using Tables\n",
    "using Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4300640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of low-yield samples: 222; Number of high-yield samples: 222\n"
     ]
    }
   ],
   "source": [
    "original_dat = CSV.read(\"../processed-data/otu-yield-per-plant.csv\", header=true, DataFrame)\n",
    "generated_dat = CSV.read(\"../processed-data/generated-data.csv\", header=false, DataFrame)\n",
    "\n",
    "original = Matrix(original_dat)\n",
    "generated = Matrix(generated_dat)\n",
    "full_data = vcat(original, generated)\n",
    "\n",
    "otu = full_data[:,2:60]\n",
    "label = full_data[:,62]\n",
    "\n",
    "percentage = [(i, count(==(i), label)) for i in unique(label)]\n",
    "println(\"Number of low-yield samples: \", percentage[1][2], \"; Number of high-yield samples: \", percentage[2][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cec15f60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "make_fold (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function make_fold(data, idx)\n",
    "    # The 2D array for data in each folder.The dimension is 22*2394\n",
    "    data_batch = Array{Float32, 2}(undef, length(idx), length(data[1,:]))\n",
    "    # Add all data for this folder into the batch\n",
    "    for i in 1:length(idx)\n",
    "        data_batch[i,:] = data[idx[i],:]\n",
    "    end\n",
    "    return (data_batch', data_batch')\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff74b2d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "k_fold_partition (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function k_fold_partition(otu_batch)\n",
    "    # partition the whole dataset into 10 folds\n",
    "    fold_idx = partition(1:length(otu_batch[:,1]), length(otu_batch[:,1])÷10+1)\n",
    "    # call make_fold and store the 10 folds\n",
    "    whole_set = [make_fold(otu_batch, i) for i in fold_idx]\n",
    "\n",
    "    return whole_set\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7983673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  Chain(\n",
       "    Dense(59 => 38, σ),                 \u001b[90m# 2_280 parameters\u001b[39m\n",
       "    Dense(38 => 14, σ),                 \u001b[90m# 546 parameters\u001b[39m\n",
       "    Dense(14 => 5),                     \u001b[90m# 75 parameters\u001b[39m\n",
       "  ),\n",
       "  Chain(\n",
       "    Dense(5 => 14, σ),                  \u001b[90m# 84 parameters\u001b[39m\n",
       "    Dense(14 => 38, σ),                 \u001b[90m# 570 parameters\u001b[39m\n",
       "    Dense(38 => 59, σ),                 \u001b[90m# 2_301 parameters\u001b[39m\n",
       "  ),\n",
       ") \u001b[90m                  # Total: 12 arrays, \u001b[39m5_856 parameters, 23.812 KiB."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Chain(\n",
    "    Chain(\n",
    "    Dense(59, 38, σ),\n",
    "    Dense(38, 14, σ),\n",
    "    Dense(14, 5)),\n",
    "    Chain(\n",
    "    Dense(5, 14, σ),\n",
    "    Dense(14, 38, σ),\n",
    "    Dense(38, 59, σ))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0891f9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function train(whole_set) \n",
    "    uni_best = 1\n",
    "    # loop through all 10 folders\n",
    "    for k in 1:10 \n",
    "        println(\"Start training on the \", k, \"th fold...\")\n",
    "        # set the training set and the testing set\n",
    "        train_set = whole_set[Not(k)]\n",
    "        test_set = whole_set[k]\n",
    "        # reset all the parameters\n",
    "        Flux.loadparams!(model, map(p -> p .= randn.(), Flux.params(model)))\n",
    "        # here the loss function is MSE, I also tried cross-entropy. \n",
    "        # I'll write down the result for both in the conclusion section\n",
    "        loss(x, y) = Flux.Losses.mse(model(x),y)\n",
    "        # record the number of continuous epoches that the loss increases\n",
    "        loss_inc = 0\n",
    "        # the loss of the current epoch for the validation set\n",
    "        val_loss = 1000\n",
    "        # the lowest loss so far in this fold\n",
    "        best_loss = 1000\n",
    "\n",
    "        # the optimizer is Adam with learning rate of 0.001\n",
    "        opt = ADAM(0.001)\n",
    "        # the maximum epoch is 200\n",
    "        for epoch_idx in 1:20000\n",
    "            # train the network\n",
    "            Flux.train!(loss, params(model), train_set, opt)\n",
    "            # calculate the validation loss for this epoch\n",
    "            val_loss = loss(test_set...)\n",
    "            # if the loss increases, increment the counter\n",
    "            if val_loss >= best_loss \n",
    "                loss_inc += 1\n",
    "            else\n",
    "                # if not, then set the current loss as lowest\n",
    "                best_loss = val_loss\n",
    "                loss_inc = 0\n",
    "                if best_loss < uni_best\n",
    "                    uni_best = best_loss\n",
    "                    BSON.@save \"AENN.bson\" model epoch_idx uni_best\n",
    "                end\n",
    "            end\n",
    "            # loss has increased for 5 continuous epoch, exit to prevent overfitting\n",
    "            if loss_inc >= 10 && opt.eta > 1e-10\n",
    "                opt.eta /= 10.0\n",
    "                loss_inc = 0\n",
    "            end\n",
    "            \n",
    "            if loss_inc >= 10\n",
    "                println(\"Force exit to prevent overfit at epoch: \", epoch_idx)\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "        println(\"Finished training the \", k, \"th folder...\" )\n",
    "        println(\"The final validation loss is: \", best_loss)\n",
    "        println(\"------------------------------------\")\n",
    "    end\n",
    "    return uni_best\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4cb8a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The untrained loss for each folder is: 0.41335514, 0.41373044, 0.4141838, 0.4136746, 0.4136541, 0.41344824, 0.41385734, 0.4136202, 0.4137717, 0.41348296, Start training...\n",
      "Start training on the 1th fold...\n",
      "Force exit to prevent overfit at epoch: 1728\n",
      "Finished training the 1th folder...\n",
      "The final validation loss is: 2.5368368e-5\n",
      "------------------------------------\n",
      "Start training on the 2th fold...\n",
      "Force exit to prevent overfit at epoch: 5116\n",
      "Finished training the 2th folder...\n",
      "The final validation loss is: 9.099798e-6\n",
      "------------------------------------\n",
      "Start training on the 3th fold...\n",
      "Force exit to prevent overfit at epoch: 4172\n",
      "Finished training the 3th folder...\n",
      "The final validation loss is: 1.3373812e-5\n",
      "------------------------------------\n",
      "Start training on the 4th fold...\n",
      "Force exit to prevent overfit at epoch: 3811\n",
      "Finished training the 4th folder...\n",
      "The final validation loss is: 1.8813957e-5\n",
      "------------------------------------\n",
      "Start training on the 5th fold...\n",
      "Force exit to prevent overfit at epoch: 1730\n",
      "Finished training the 5th folder...\n",
      "The final validation loss is: 2.1745434e-5\n",
      "------------------------------------\n",
      "Start training on the 6th fold...\n",
      "Force exit to prevent overfit at epoch: 2864\n",
      "Finished training the 6th folder...\n",
      "The final validation loss is: 1.1590897e-5\n",
      "------------------------------------\n",
      "Start training on the 7th fold...\n",
      "Force exit to prevent overfit at epoch: 7345\n",
      "Finished training the 7th folder...\n",
      "The final validation loss is: 7.299021e-6\n",
      "------------------------------------\n",
      "Start training on the 8th fold...\n",
      "Force exit to prevent overfit at epoch: 7453\n",
      "Finished training the 8th folder...\n",
      "The final validation loss is: 8.214204e-6\n",
      "------------------------------------\n",
      "Start training on the 9th fold...\n",
      "Force exit to prevent overfit at epoch: 2614\n",
      "Finished training the 9th folder...\n",
      "The final validation loss is: 1.0864528e-5\n",
      "------------------------------------\n",
      "Start training on the 10th fold...\n",
      "Force exit to prevent overfit at epoch: 1486\n",
      "Finished training the 10th folder...\n",
      "The final validation loss is: 2.2520095e-5\n",
      "------------------------------------\n",
      "The lowest loss among all models is: 7.299021e-6\n"
     ]
    }
   ],
   "source": [
    "otu = otu[shuffle(1:size(otu)[1]), :]\n",
    "whole_set = k_fold_partition(otu)\n",
    "# reset the parameter of the model to get the untrained loss, just as a reference\n",
    "Flux.loadparams!(model, map(p -> p .= randn.(), Flux.params(model)))\n",
    "loss(x, y) = Flux.Losses.mse(model(x),y)\n",
    "print(\"The untrained loss for each folder is: \")\n",
    "for i in 1:10\n",
    "    print(loss(whole_set[i]...), \", \")\n",
    "end\n",
    "\n",
    "# Start the training\n",
    "println(\"Start training...\")\n",
    "best_loss = train(whole_set)\n",
    "println(\"The lowest loss among all models is: \", best_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0aad0ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "BSON.@load \"AENN.bson\" model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69054774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"../processed-data/plot.csv\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "while true\n",
    "    rand_num = rand((1:size(otu)[1]),6)\n",
    "    if length(unique(rand_num)) == 6\n",
    "        break\n",
    "    end\n",
    "end\n",
    "\n",
    "a_x = otu[rand_num[1],:]\n",
    "a_y = model(a_x)\n",
    "b_x = otu[rand_num[2],:]\n",
    "b_y = model(b_x)\n",
    "c_x = otu[rand_num[3],:]\n",
    "c_y = model(c_x)\n",
    "d_x = otu[rand_num[4],:]\n",
    "d_y = model(d_x)\n",
    "e_x = otu[rand_num[5],:]\n",
    "e_y = model(e_x)\n",
    "f_x = otu[rand_num[6],:]\n",
    "f_y = model(f_x)\n",
    "\n",
    "whole = hcat(a_x, a_y, b_x, b_y, c_x, c_y, d_x, d_y, e_x, e_y, f_x, f_y)\n",
    "CSV.write(\"../processed-data/plot.csv\",  Tables.table(c), writeheader=false)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.3",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
