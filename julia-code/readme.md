# Julia code
This .md file gives an overview of the Julia code used in this study and instruction for setting up the Julia environment to reproduce the results of the study. The Julia code is mainly used for pre and post process of the Bayesian Neural Network (BNN) models as the actual models are run by shell scripts in Linux terminal. The Julia code also performs data-augmentation, which is used in both BNN models and python scripts.

## File description
All the following files should finish running in less than 5 minutes.

|File name|Description|
|---|---|
|`data-augmentation.ipynb`|The code for data augmentation of raw count OTUs from the original data.|
|`file-process.ipynb`|The code for pre-processing of BNN models. It combine normalized/selected data from `NetComi` and `python-code` with binarized labels. The train-test-split is done in R as no realiable spliting package was found in Julia when the code is written.
|`result-computation.ipynb`| The code for post-processing of BNN models. It reads in the .txt files produced by the shell scripts and produce a table of measurements identical to the python scripts for Random-Forest models.|

## Enivornment setup
The codes are compiled in Julia 1.8.2, this version or later versions are available on [the Julia webpage](https://julialang.org/downloads/). Note that versions prior to 1.8.2 will not work for those scripts.

Once installed the appropriate Julia version, the `IJulia` package is needed to open the `Jupyter Notebook`. The instruction for installing and using the package is available on [the `IJulia` page](https://julialang.github.io/IJulia.jl/stable/manual/installation/).

Some additional packages are needed for running all the code: `CSV`, `DataFrames`, `XLSX`,`Statistics`, `Distributions`, `Random`, `Tables`, `Glob`, `MLJ`, `MLJBase`, `DelimitedFiles`, and `CategoricalArrays`. Those packages could be easily added in the Julia terminal.

The folders that are needed for those code to run are described in the next section.

Once the folders are set up, the code could be run in the `IJulia` interface easily.

## Folders needed
In the same directory of your folder that stores these Julia codes, you need to create two folders: `raw-data` and `processed-data`.

In the `raw-data` folder, store all the raw `.csv` data provided in it.

For the `processed-data` folder, a compressed file system is provided in [this link](https://drive.google.com/file/d/1wYtYQLl1D24PlZQHlNYWvLt23ewa3oow/view?usp=sharing). The user need to decompress it and put the appropriate files into the corresponding folder. Detailed description of those folders and workflow will be provided in the next section.


## Workflow
The workflow of the Julia codes are as follows:

Run `data-augmentation.ipynb` first before all other scripts in this repository as it be solely based on the original raw data. Note that you need to run part of the `train-test-split.R` in-between `data-augmentation.ipynb`. The cut-point is clearly marked in both scripts, follow the instruction provided in these to files.

After running `data-augmentation.ipynb`, the full augmented OTU data will be in `./processed-data/data-augmentation/otu/` with all 5 levels. The full binary responses will be in `./processed-data/data-augmentation/response/`, with first 800 rows being training set and the rest being testing. The ID of each row is unique, so merging OTU and response is easy. The files in those 2 folders will be needed to run normalization codes in `NetComi` and RF models in `python-code`.

Next, run `NetComi` in the `r-code` and feature selection codes in `python-code` (you can also run RF models in `Python-code` first, as they will not interfere with this part). Put all normalized predictors in their corresponding folder in `processed-data` as below:

|Folder name| Description| Note|
|---|---|---|
|`all_otu_augmented`| Augmented OTUs of 5 levels with normalization and zero treatment| Put the normalized data in `full-data`, with the same 5 folder name|
|`all_otu_non_augmented`| Original OTUs of 5 levels with normalization and zero treatment| Put the normalized data in `original`, with the same 5 folder name|
|`alpha_index_data`| Alpha diversity indices of 5 levels with normalization| Put the normalized data in `original`, with the same 5 folder name|
|`alpha_soil`| Alpha diversity indices of 5 levels and soil chemistry, with normalization| Leave as it is|
|`alpha_soil_disease`|Alpha diversity indices of 5 levels, soil chemistry, and disease suppression abilities, with normalization| Leave as it is|
|`disease_suppression_data`| Disease suppression abilities with normalization |Put the normalized data in `original`|
|`otu_disease`|OTU with feature selection score=3 normalized by row sum, and disease suppression abilities with normalization |Leave as it is|
|`otu_selection`|OTU with feature selection score from 0 to 4, normalized by row sum| Put the feature selection table generated by `python-code` under this folder|
|`otu_soil`| OTU with feature selection score=3 normalized by row sum, and soil chemistry with normalization| Leave as it is|
|`otu_soil_disease`|OTU with feature selection score=3 normalized by row sum, soil chemistry, and disease suppression abilities with normalization|Leave as it is|
|`soil_chemistry_data`| soil chemistry with normalization| Put the normalized data in `original`|
|`soil_disease`| soil chemistry and disease suppression abilities with normalization|Leave as it is|

Run `file-process.ipynb` after the `NetComi` packages and the python scripts, and before all shell scripts and `train-test-split.R`. This code is based on the normalized/selected data from other scripts, and produce the data to train and test for BNN models. After finished running `file-process.ipynb`, run the second portion of `train-test-split.R` in `r-code` folder, marked by highlighted comment in the code. The training and testing dataset are stored in the inner most`train-test-split` folder of each folder mentioned above. Use those files in the shell scripts, detailed description could be found in the readme file of `shell-script` folder.

Each shell script runs a BNN model, and produce a `.txt` file as results. Put all `.txt` files into the corresponding folders in `./processed-data/result_file` as the folder name suggests. Then, run `result-computation.ipynb` and the final measurements of performance for each response will be saved in `ALL_response` folder under `result_file` in the format of a spreadsheet.
